Итоговая документация: От идеи до RL-агента, торгующего на новостях
Этот документ — полный конспект нашего с вами пути по созданию продвинутой торговой системы. Мы начали с простой идеи, столкнулись с реальными техническими и методологическими проблемами и пришли к проектированию системы профессионального уровня.

Раздел 1: Начало — Оптимизация GRU-модели (ETH)
Где мы были: У нас был базовый скрипт для бэктестинга GRU-модели, который показывал убытки.

Проблема №1: Убытки из-за комиссий.

Диагноз: Несмотря на приемлемый процент прибыльных сделок, стратегия совершала слишком много сделок, и затраты на комиссии "съедали" всю прибыль.

Решение: Мы внедрили фильтры (по тренду, волатильности, объему) и механизмы управления рисками (лимит сделок в день, стоп по просадке), чтобы повысить качество сигналов и ограничить избыточную торговлю.

Проблема №2: Низкая производительность.

Диагноз: Оптимизация на мощном Mac M3 Pro занимала неоправданно много времени. Причина — использование только одного ядра процессора.

Решение: Мы включили параллельные вычисления, добавив n_jobs=-1 в Optuna. Это привело к следующему открытию.

Проблема №3: "Бутылочное горлышко" GPU.

Наблюдение: Ускорение при использовании всех ядер CPU оказалось значительно выше, чем при использовании GPU.

Диагноз: Множество параллельных процессов "боролись" за доступ к одному GPU, создавая "пробку", которая нивелировала всю пользу от скорости видеокарты.

Решение: Для задач с множеством коротких, независимых вычислений (как наш бэктестинг) мы решили принудительно использовать все ядра CPU, что дало максимальное ускорение.

Раздел 2: Создание профессионального воркфлоу
Где мы были: Мы научились быстро находить "лучшие" параметры, но результаты были нестабильны.

Проблема: Переобучение (Overfitting).

Наблюдение: Стратегия, показывавшая фантастические +480% на одном наборе данных, имела коэффициент Шарпа всего 0.51.

Диагноз: Это классический признак переобучения. Стратегия не выучила рыночный принцип, а просто "запомнила" аномалии в конкретном историческом отрезке. Низкий Шарп показал, что прибыль была крайне нестабильной и рискованной.

Решение: Мы внедрили профессиональный Train/Validate/Test воркфлоу. Этот процесс позволяет обучать модель на старых данных, проверять ее на "недавних" и проводить финальный, честный "экзамен" на самых свежих данных, которых модель никогда не видела.

Раздел 3: Фундаментальное улучшение модели
Где мы были: Наш профессиональный воркфлоу показал, что старая GRU-модель не может стабильно зарабатывать, и мы уперлись в "плато" производительности.

Проблема: Предел возможностей "мозга".

Диагноз: Модель, которая видит только 2 индикатора (RSI и VWAP), фундаментально ограничена в своей способности понимать рынок. Подбор внешних параметров (buy_threshold и т.д.) больше не давал улучшения.

Решение: Сделать "мозг" умнее. Мы создали новую V2 модель, переписав скрипт обучения (train_gru_scalper_v2.py) так, чтобы он использовал 5 индикаторов (RSI, VWAP, Stochastic, CCI, Price Change).

Результат: Новая V2 модель на обучении показала высочайшую точность (F1-score > 0.97), доказав, что более полная картина рынка позволяет делать значительно более точные прогнозы.

Раздел 4: Проектирование RL-агента (Золото и Новости)
Где мы были: Мы создали очень точную модель-предсказатель, но решили пойти дальше и создать модель, которая принимает решения, а не просто предсказывает.

Задача: Создать RL-агента, который торгует золотом на основе новостей и индикаторов.

Проблема №1: Источники данных.

Диагноз: yfinance и бесплатные новостные API (NewsAPI.org) имеют слишком жесткие ограничения на глубину исторической информации.

Решение: Мы перешли на профессиональные инструменты: API брокера Capital.com для получения качественных цен и платный новостной API (MarketAux) для получения релевантных финансовых новостей.

Проблема №2: Отладка API.

Диагноз: В процессе интеграции мы столкнулись с типичными ошибками: неверные имена переменных в .env, конфликты временных зон (tz-aware vs tz-naive), некорректные параметры запросов (max_points).

Решение: Мы последовательно отладили каждую из этих проблем, создав надежный конвейер данных (data_pipeline.py), который собирает, обрабатывает и объединяет данные из разных источников.

Раздел 5: Текущий этап — Обучение RL-агента
Где мы сейчас: Мы успешно создали качественный набор данных по золоту за год и запустили обучение нашего RL-агента (rl_gold_trader.py) на облачном GPU.

Наблюдение: Агент на первых 150+ эпизодах показывает стабильно отрицательную награду (Reward) и совершает очень много сделок (Trades).

Анализ: Это нормальный и необходимый этап обучения. Агент начинает с "чистого листа" и с помощью хаотичных действий исследует (exploration) среду, чтобы понять базовые принципы. Сейчас он учится не зарабатывать, а не терять — он получает отрицательное подкрепление за огромное количество сделок и связанные с ними комиссии.

Улучшение: Мы заметили, что обучение шло слишком медленно, и внесли улучшение в функцию вознаграждения (Reward Function), чтобы давать агенту более четкие сигналы о том, что такое хорошая и плохая сделка.

Раздел 6: Что дальше?
Наш следующий шаг — дождаться завершения обучения текущего RL-агента.

Завершить обучение: Дать агенту пройти 500+ эпизодов, наблюдая за динамикой: сначала должно сократиться количество сделок, а затем отрицательная награда должна начать двигаться к нулю.

Тестирование: Сохранить обученный "мозг" агента (.pth файл) и протестировать его на данных, которые он не видел, чтобы получить честную оценку его производительности.

Paper Trading: Запустить бота на демо-счете Capital.com, чтобы проверить его поведение в реальном времени.

Дальнейшие улучшения: Если результаты будут многообещающими, можно будет внедрять еще более сложные концепции, которые мы обсуждали, например, Скрытые Марковские Модели (HMM) для определения рыночных режимов.