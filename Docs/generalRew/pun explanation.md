Анализ Функций Вознаграждения в Reinforcement Learning
В Reinforcement Learning (RL) агент учится, получая "награды" и "штрафы" за свои действия. Проектирование этой системы — это искусство, и то, как мы ее строим, напрямую влияет на то, научится ли агент чему-то полезному или будет "топтаться на месте".

Мы с вами попробовали два разных подхода. Давайте разберем их логику, плюсы и минусы.

Модель №1: Частая, "Направляющая" Награда (Та, что показала прогресс)
Это версия, которая находится в вашем коде сейчас. Она показала результат Reward: 33.49 | Final Balance: 12733.27. Ее ключевая особенность — агент получает обратную связь почти на каждом шаге.

Логика работы (из функции step):

Награда за удержание позиции (главный "учитель"):

# Награда за удержание позиции
if self.position == 1:
    if (self.current_step + 1) < len(self.data):
        next_price = self.data['Close'].iloc[self.current_step + 1]
        price_diff = next_price - current_price
        # Награда пропорциональна изменению цены, пока позиция открыта
        reward += (price_diff * self.position_size) / self.initial_balance * 10

Что это значит: На каждом часе, пока позиция открыта, агент получает небольшую награду, если цена движется в его пользу, и небольшой штраф, если цена идет против него.

Зачем это нужно: Это называется "формирование награды" (Reward Shaping). Мы не ждем конца сделки, чтобы сказать агенту, прав он или нет. Мы даем ему постоянные, маленькие "подсказки" или "намеки". Это помогает ему гораздо быстрее понять, какие ситуации ведут к росту, а какие — к падению.

Награда за закрытие сделки:

elif action == 2 and self.position == 1: # Sell to close
    profit = (current_price - self.entry_price) * self.position_size
    # ...
    reward += profit / self.initial_balance * 10 # Награда пропорциональна прибыли

Что это значит: Когда сделка наконец закрывается, агент получает большую награду (или штраф), пропорциональную итоговому финансовому результату. Это его главная цель.

Штраф за открытие сделки:

if action == 1 and self.position == 0: # Buy
    # ...
    reward -= 0.1 # Штраф за открытие

Что это значит: Небольшой штраф за вход в позицию. Это учит агента не совершать сделки без веской причины, так как каждая сделка "стоит" ему немного награды.

Плюсы этой модели:
Плотная обратная связь: Агент постоянно получает сигналы, что хорошо, а что плохо. Это значительно ускоряет обучение.

Эффективное обучение: "Подсказки" помогают агенту быстрее находить правильные закономерности и не застревать в цикле случайных действий.

Минусы:
Риск "близорукости": Если неправильно настроить, агент может научиться гнаться за сиюминутными небольшими наградами, игнорируя долгосрочную стратегию. Но в нашем случае баланс подобран хорошо.

Модель №2: Редкая, "Целевая" Награда (Та, что "топталась на месте")
Это была промежуточная версия, где агент получал награду только в момент закрытия сделки.

Логика работы:

Награда за удержание: Отсутствует (reward = 0).

Награда за закрытие: Большая фиксированная награда +10 за прибыльную сделку и большой штраф -10 за убыточную.

Плюсы этой модели:
Простота: Логика очень проста и понятна. Цель ясна — закрыть сделку в плюс.

Минусы (и почему она не сработала):
Проблема "разреженных наград" (Sparse Rewards): Это главная причина провала. Представьте, что агент открывает сделку. Следующие 50 часов он держит позицию. За все это время он не получает никакой обратной связи. Ему неизвестно, было ли его решение открыть сделку хорошим или плохим. Он не понимает, стоит ли ему продолжать держать позицию или нет.

Сложность установления причинно-следственных связей: Поскольку награда приходит очень редко, агенту крайне сложно понять, какое именно из сотен его предыдущих действий привело к финальному результату. Он не может отличить удачу от мастерства.

"Паралич" от исследования: Не получая никаких "подсказок", агент продолжает совершать случайные действия, которые в 99% случаев приводят к убытку из-за комиссий, и он никогда не добирается до стадии, когда его действия начинают приносить положительный результат.

Итоговый вывод
Модель, которая показала прогресс, работает лучше, потому что она использует гибридный подход. Она ставит перед агентом глобальную цель (заработать на сделке), но при этом помогает ему на пути к этой цели с помощью маленьких, частых "подсказок". Это самый эффективный способ обучения для сложных задач с отложенным результатом, какой и является трейдинг.