The Algotrading Project: A Full-Stack Roadmap
This document outlines the comprehensive, multi-phase plan for our project. We have evolved from a simple backtest into the development of a sophisticated, AI-driven trading system. This is our structured roadmap.

Phase I: Foundation - Supervised Learning (GRU Model)
Status: ‚úÖ Complete

Objective: To build and validate a foundational trading strategy based on a predictive model. This phase taught us the core mechanics of backtesting, optimization, and the dangers of overfitting.

Key Outcome

Description

Professional Backtester

We built a robust backtesting script (professional_backtester.py) that uses a Train/Validate/Test methodology to provide honest, reliable performance metrics and protect against overfitting.

Optimized V2 Model

We created an advanced GRU model (train_gru_scalper_v2.py) that uses 5 technical indicators (RSI, VWAP, Stoch, CCI, Price Change) and demonstrated extremely high predictive accuracy (F1 Score > 0.97).

Performance Discovery

We identified that for our specific task (mass parallel backtests), a multi-core CPU setup on your local machine is significantly faster than a cloud-based GPU, avoiding I/O bottlenecks.

Phase II: Core Development - Reinforcement Learning (RL Agent)
Status: üü° In Progress

Objective: To move beyond simple prediction and create a true AI agent that learns an optimal decision-making policy for trading Gold based on both technicals and news sentiment.

Sub-Step

Description

Status

2.1: The Data Pipeline

We built a robust data pipeline (data_pipeline.py) that gathers a full year of hourly price data from Capital.com and high-quality financial news from the MarketAux API. It then merges them into a single, clean dataset.

‚úÖ Complete

2.2: RL Agent Training

This is our current active step. We are training a PPO agent (rl_gold_trader.py) on our enriched dataset using a cloud GPU on Google Colab.

üü° Active

2.3: Reward Function Tuning

We observed the agent "stalling" and correctly identified the need to refine the reward function. We have iterated to find a balance that encourages exploration without excessive, random trading. The "guiding hand" approach (frequent small rewards) has proven most effective.

‚úÖ Complete

2.4: Hyperparameter Tuning

Next immediate action. To solve the "stalling" problem permanently, we will use Optuna to find the best learning parameters (learning rate, network size, etc.) for the RL agent itself by running many training sessions in parallel. This will produce the most effective final model.

‚è≥ Next Up

Phase III: Validation & Deployment
Status: ‚è≥ Upcoming

Objective: To test our trained RL agent in a live environment and prepare it for real-world trading.

Sub-Step

Description

3.1: Live Paper Trading (Demo)

We will deploy our best trained RL agent using the PyQt6 Dashboard we designed. This bot will connect to your Capital.com demo account and execute trades in real-time, allowing us to see its performance without financial risk.

3.2: Performance Review

After a set period of paper trading (e.g., 1-2 weeks), we will analyze the results. Does the live performance match the backtest? Is the strategy profitable? What are its weaknesses?

3.3: Real Trading

If the paper trading results are positive and stable, the final step is deployment. This involves connecting the bot to a real Capital.com account with a small, controlled amount of capital and strict risk management.

Phase IV: Future Research & Advanced Concepts
Status: üí° Ideas

Objective: To explore cutting-edge techniques to make our agent even more intelligent and robust.

Concept

Description

Hidden Markov Models (HMM)

We can train an HMM to identify the current market "regime" (e.g., bull market, bear market, sideways). We can then feed this regime information to our RL agent, allowing it to use different strategies for different market conditions.

Attention Mechanisms

We can upgrade our model's neural network architecture with an "attention" layer. This would allow the agent to pay more attention to the most important historical data points when making a decision.

Multi-Asset & Portfolio Mgmt.

Once we have a profitable agent for Gold, we can expand the system. We can train separate agents for other assets (like EUR/USD) or a single, more complex agent that learns to manage an entire portfolio, allocating capital between different assets.

