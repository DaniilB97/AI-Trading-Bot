import optuna
import logging
import pandas as pd
import torch
import os
import json

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–∑ –≤–∞—à–µ–≥–æ —Ä–∞–±–æ—á–µ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞
from rl_gold_traderv0_01 import NewsTradingEnvironment, PPOAgent, Memory, ActorCritic

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- –û–ë–™–ï–ö–¢–ò–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –î–õ–Ø OPTUNA ---
# –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –±—É–¥–µ—Ç –∑–∞–ø—É—Å–∫–∞—Ç—å –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –∏ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç,
# –∫–æ—Ç–æ—Ä—ã–π Optuna –±—É–¥–µ—Ç –ø—ã—Ç–∞—Ç—å—Å—è –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å.
def objective(trial, data_df):
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–∏–Ω –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è RL-–∞–≥–µ–Ω—Ç–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –µ–≥–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å.
    """
    logger.info(f"\n===== Starting Optuna Trial #{trial.number} =====")
    
    # 1. Optuna –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —ç—Ç–æ–≥–æ —Ç–µ—Å—Ç–∞
    params = {
        "lr": trial.suggest_float("lr", 1e-5, 1e-3, log=True),
        "gamma": trial.suggest_float("gamma", 0.9, 0.999),
        "K_epochs": trial.suggest_int("K_epochs", 4, 10),
        "update_timestep": trial.suggest_int("update_timestep", 1000, 5000, step=500),
        "hidden_dim": trial.suggest_categorical("hidden_dim", [128, 256, 512])
    }
    logger.info(f"Trial Parameters: {params}")

    # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–µ–¥—ã –∏ –∞–≥–µ–Ω—Ç–∞ —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π lookback_window –∏ commission –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
    env = NewsTradingEnvironment(data_df, lookback_window=30, commission=0.001)
    state_dim = env.observation_space.shape[0]
    action_dim = 3 # Hold, Buy, Sell
    
    agent = PPOAgent(
        state_dim, 
        action_dim, 
        lr=params["lr"], 
        gamma=params["gamma"], 
        K_epochs=params["K_epochs"]
    )
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º ActorCritic —Å –Ω–æ–≤—ã–º hidden_dim
    agent.policy = ActorCritic(state_dim, action_dim, hidden_dim=params["hidden_dim"]).to(agent.device)
    agent.policy_old = ActorCritic(state_dim, action_dim, hidden_dim=params["hidden_dim"]).to(agent.device)
    agent.policy_old.load_state_dict(agent.policy.state_dict())
    agent.optimizer = torch.optim.Adam(agent.policy.parameters(), lr=params["lr"])


    memory = Memory()
    
    # 3. –ó–∞–ø—É—Å–∫ —É–∫–æ—Ä–æ—á–µ–Ω–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ—Ü–µ–Ω–∫–∏
    # –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω—å—à–µ —ç–ø–∏–∑–æ–¥–æ–≤, —á—Ç–æ–±—ã –æ–¥–∏–Ω —Ç–µ—Å—Ç –Ω–µ –∑–∞–Ω–∏–º–∞–ª 10 —á–∞—Å–æ–≤
    num_episodes = 100 # –£–º–µ–Ω—å—à–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ—Ü–µ–Ω–∫–∏
    timestep = 0
    final_balance = 0
    
    for episode in range(1, num_episodes + 1):
        state, _ = env.reset()
        done = False
        while not done:
            timestep += 1
            action = agent.select_action(state, memory)
            state, reward, done, _, _ = env.step(action)
            memory.rewards.append(reward)
            memory.is_terminals.append(done)
            
            if timestep % params["update_timestep"] == 0:
                agent.update(memory)
                memory.clear()

        final_balance = env.balance
        # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–π 10-–π —ç–ø–∏–∑–æ–¥, —á—Ç–æ–±—ã –Ω–µ –∑–∞—Å–æ—Ä—è—Ç—å –≤—ã–≤–æ–¥
        if episode % 10 == 0:
            logger.info(f"Trial #{trial.number}, Episode {episode} | Final Balance: {final_balance:.2f}")

    logger.info(f"===== Finished Optuna Trial #{trial.number} | Final Balance: {final_balance:.2f} =====\n")
    
    # 4. –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å. Optuna –±—É–¥–µ—Ç –ø—ã—Ç–∞—Ç—å—Å—è –µ–≥–æ –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å.
    return final_balance


# --- –ì–õ–ê–í–ù–´–ô –ë–õ–û–ö –ó–ê–ü–£–°–ö–ê ---
if __name__ == '__main__':
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ–¥–∏–Ω —Ä–∞–∑
    try:
        data_df = pd.read_csv('gold_data_with_sentiment_hourly.csv', index_col='Datetime', parse_dates=True)
        logger.info(f"Successfully loaded dataset with shape: {data_df.shape}")
    except FileNotFoundError:
        logger.error("‚ùå 'gold_data_with_sentiment_hourly.csv' not found. Please run data_pipeline.py first.")
        exit()

    # 2. –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ Optuna
    # n_jobs=-1 –∑–∞–¥–µ–π—Å—Ç–≤—É–µ—Ç –≤—Å–µ —è–¥—Ä–∞ –≤–∞—à–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤
    study = optuna.create_study(direction="maximize")
    study.optimize(lambda trial: objective(trial, data_df), n_trials=50, n_jobs=-1)
    
    # 3. –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    logger.info("\nüéâ Optuna Optimization Finished! üéâ")
    logger.info(f"Best trial number: {study.best_trial.number}")
    logger.info(f"Best value (Final Balance): ${study.best_trial.value:.2f}")
    logger.info("Best hyperparameters found:")
    for key, value in study.best_trial.params.items():
        logger.info(f"  - {key}: {value}")
        
    # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ —Ñ–∞–π–ª
    with open("best_rl_hyperparams.json", "w") as f:
        json.dump(study.best_trial.params, f, indent=2)
    logger.info("‚úÖ Best hyperparameters saved to best_rl_hyperparams.json")
