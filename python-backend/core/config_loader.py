# extended_data_pipeline.py - –ü–æ–ª—É—á–µ–Ω–∏–µ 3+ –ª–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

import os
import pandas as pd
import requests
import logging
from datetime import datetime, timedelta, timezone
from dotenv import load_dotenv
import time
import sys
from typing import Dict, List, Optional
import numpy as np
import pickle

# –ò–º–ø–æ—Ä—Ç –∏–∑ core
sys.path.append(os.path.join(os.path.dirname(os.path.dirname(__file__)), 'core'))
from core.capital_request import CapitalComAPI

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

load_dotenv()

class ExtendedDataPipeline:
    """–ü–∞–π–ø–ª–∞–π–Ω –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.capital_api = CapitalComAPI(
            os.getenv("CAPITAL_API_KEY"),
            os.getenv("CAPITAL_IDENTIFIER"),
            os.getenv("CAPITAL_PASSWORD")
        )
        self.marketaux_api_key = os.getenv("MARKETAUX_API_TOKEN")
        
        # –¶–µ–ª–µ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö
        self.instruments = {
            'primary': ['GOLD'],  # –û—Å–Ω–æ–≤–Ω–æ–π –∞–∫—Ç–∏–≤
            'correlations': ['DXY', 'VIX', 'SPX500', 'EUR/USD', 'GBP/USD'],  # –ö–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –∞–∫—Ç–∏–≤—ã
            'alternatives': ['SILVER', 'OIL', 'NATGAS', 'COPPER']  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã
        }
        
    def get_maximum_historical_data(self, years_back: int = 3) -> pd.DataFrame:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –æ–±—ä–µ–º–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ —Å–±–æ—Ä–∞ {years_back} –ª–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        if not self.capital_api.login_and_get_tokens():
            logger.error("‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ Capital.com")
            return pd.DataFrame()
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤–æ–∑–º–æ–∂–Ω—ã–µ
        end_date = datetime.now(timezone.utc)
        start_date = end_date - timedelta(days=years_back * 365)
        
        all_data_frames = []
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –≤—Å–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º
        all_instruments = []
        for category, instruments in self.instruments.items():
            all_instruments.extend(instruments)
        
        for instrument in all_instruments:
            logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∫–∞ {instrument}...")
            
            try:
                # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –±–æ–ª—å—à–∏–º–∏ –∫—É—Å–∫–∞–º–∏ (–ø–æ 6 –º–µ—Å—è—Ü–µ–≤)
                instrument_df = self._get_instrument_data_in_chunks(
                    instrument, start_date, end_date, chunk_months=6
                )
                
                if not instrument_df.empty:
                    all_data_frames.append(instrument_df)
                    logger.info(f"   ‚úÖ {instrument}: {len(instrument_df)} –∑–∞–ø–∏—Å–µ–π")
                else:
                    logger.warning(f"   ‚ö†Ô∏è {instrument}: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
                    
            except Exception as e:
                logger.error(f"   ‚ùå –û—à–∏–±–∫–∞ {instrument}: {e}")
                continue
        
        if not all_data_frames:
            logger.error("‚ùå –ù–µ –ø–æ–ª—É—á–µ–Ω–æ –Ω–∏–∫–∞–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö")
            return pd.DataFrame()
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        logger.info("üîÑ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö...")
        combined_df = pd.concat(all_data_frames, axis=1)
        combined_df.sort_index(inplace=True)
        
        # Forward fill –¥–ª—è –≤—ã—Ö–æ–¥–Ω—ã—Ö –∏ –ø—Ä–∞–∑–¥–Ω–∏–∫–æ–≤
        combined_df = combined_df.ffill()
        
        logger.info(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(combined_df)} –∑–∞–ø–∏—Å–µ–π –∑–∞ {years_back} –ª–µ—Ç")
        return combined_df
    
    def _get_instrument_data_in_chunks(self, instrument: str, start_date: datetime, 
                                       end_date: datetime, chunk_months: int = 6) -> pd.DataFrame:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—É –±–æ–ª—å—à–∏–º–∏ –∫—É—Å–∫–∞–º–∏"""
        
        chunks = []
        current_start = start_date
        
        while current_start < end_date:
            current_end = min(current_start + timedelta(days=chunk_months * 30), end_date)
            
            logger.info(f"   üì• {instrument}: {current_start.date()} ‚Üí {current_end.date()}")
            
            try:
                # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –∑–∞ –ø–µ—Ä–∏–æ–¥
                days_in_chunk = (current_end - current_start).days
                max_points = min(days_in_chunk * 24, 1000)  # Capital.com limit
                
                price_data = self.capital_api.get_historical_prices(
                    epic=instrument,
                    resolution="HOUR",  # –ß–∞—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
                    max_points=max_points,
                    from_date=current_start.strftime('%Y-%m-%dT%H:%M:%S'),
                    to_date=current_end.strftime('%Y-%m-%dT%H:%M:%S')
                )
                
                if price_data and 'prices' in price_data and price_data['prices']:
                    chunk_df = self._convert_price_data_to_df(price_data, instrument)
                    if not chunk_df.empty:
                        chunks.append(chunk_df)
                
                # Rate limiting
                time.sleep(2)
                
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫—É—Å–∫–∞ {instrument}: {e}")
                time.sleep(5)  # –ë–æ–ª—å—à–µ –∑–∞–¥–µ—Ä–∂–∫–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            
            current_start = current_end
        
        if chunks:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫—É—Å–∫–∏ –∏ —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            combined = pd.concat(chunks)
            combined = combined[~combined.index.duplicated(keep='first')]
            combined.sort_index(inplace=True)
            return combined
        else:
            return pd.DataFrame()
    
    def _convert_price_data_to_df(self, price_data: dict, instrument: str) -> pd.DataFrame:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ DataFrame —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
        rows = []
        
        for price_point in price_data['prices']:
            try:
                dt = pd.to_datetime(price_point['snapshotTime']).tz_localize('UTC')
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                row_data = {
                    'Datetime': dt,
                    f'{instrument}_Open': float(price_point.get('openPrice', {}).get('ask', 0)),
                    f'{instrument}_High': float(price_point.get('highPrice', {}).get('ask', 0)),
                    f'{instrument}_Low': float(price_point.get('lowPrice', {}).get('ask', 0)),
                    f'{instrument}_Close': float(price_point.get('closePrice', {}).get('ask', 0)),
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º bid/ask –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
                if 'closePrice' in price_point:
                    if 'bid' in price_point['closePrice']:
                        row_data[f'{instrument}_Bid'] = float(price_point['closePrice']['bid'])
                    if 'ask' in price_point['closePrice']:
                        row_data[f'{instrument}_Ask'] = float(price_point['closePrice']['ask'])
                
                # –û–±—ä–µ–º –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
                if 'lastTradedVolume' in price_point:
                    row_data[f'{instrument}_Volume'] = float(price_point['lastTradedVolume'])
                
                rows.append(row_data)
                
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–æ—á–∫–∏ {instrument}: {e}")
                continue
        
        if rows:
            df = pd.DataFrame(rows)
            df.set_index('Datetime', inplace=True)
            return df
        else:
            return pd.DataFrame()
    
    def get_extended_news_data(self, years_back: int = 3) -> pd.DataFrame:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –æ–±—ä–µ–º–∞ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        logger.info(f"üì∞ –°–±–æ—Ä {years_back} –ª–µ—Ç –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        if not self.marketaux_api_key:
            logger.error("‚ùå MarketAux API key –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return pd.DataFrame()
        
        end_date = datetime.now(timezone.utc)
        start_date = end_date - timedelta(days=years_back * 365)
        
        all_articles = []
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∫—É—Å–∫–∞–º–∏ –ø–æ 1 –º–µ—Å—è—Ü—É (–ª–∏–º–∏—Ç—ã API)
        current_start = start_date
        
        while current_start < end_date:
            current_end = min(current_start + timedelta(days=30), end_date)
            
            logger.info(f"üì• –ù–æ–≤–æ—Å—Ç–∏: {current_start.date()} ‚Üí {current_end.date()}")
            
            try:
                params = {
                    'api_token': self.marketaux_api_key,
                    'symbols': 'XAUUSD,DXY,SPY,GLD',  # –ó–æ–ª–æ—Ç–æ –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ
                    'filter_entities': 'true',
                    'language': 'en',
                    'published_after': current_start.strftime('%Y-%m-%dT%H:%M:%S'),
                    'published_before': current_end.strftime('%Y-%m-%dT%H:%M:%S'),
                    'limit': 100
                }
                
                response = requests.get(
                    "https://api.marketaux.com/v1/news/all", 
                    params=params, 
                    timeout=15
                )
                
                if response.status_code == 200:
                    data = response.json()
                    articles = data.get('data', [])
                    
                    if articles:
                        all_articles.extend(articles)
                        logger.info(f"   ‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π")
                    else:
                        logger.info(f"   ‚ö™ –ù–µ—Ç —Å—Ç–∞—Ç–µ–π –∑–∞ –ø–µ—Ä–∏–æ–¥")
                        
                elif response.status_code == 429:
                    logger.warning("‚ö†Ô∏è Rate limit - –∂–¥–µ–º...")
                    time.sleep(60)
                    continue
                else:
                    logger.warning(f"‚ö†Ô∏è API –æ—à–∏–±–∫–∞: {response.status_code}")
                
                # Rate limiting
                time.sleep(3)
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
                time.sleep(10)
            
            current_start = current_end
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame
        if all_articles:
            news_data = []
            for article in all_articles:
                try:
                    news_data.append({
                        'datetime': pd.to_datetime(article['published_at']),
                        'title': article.get('title', ''),
                        'description': article.get('description', ''),
                        'sentiment_score': article.get('sentiment_score', 0.0),
                        'source': article.get('source', ''),
                        'entities': str(article.get('entities', [])),
                        'url': article.get('url', '')
                    })
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç—å–∏: {e}")
                    continue
            
            if news_data:
                news_df = pd.DataFrame(news_data)
                news_df.set_index('datetime', inplace=True)
                news_df.sort_index(inplace=True)
                
                logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(news_df)} –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π")
                return news_df
        
        logger.warning("‚ö†Ô∏è –ù–æ–≤–æ—Å—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø–æ–ª—É—á–µ–Ω—ã")
        return pd.DataFrame()
    
    def create_master_dataset(self, years_back: int = 3) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å—Ç–µ—Ä-–¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        logger.info("üî• –°–æ–∑–¥–∞–Ω–∏–µ –ú–ê–°–¢–ï–†-–î–ê–¢–ê–°–ï–¢–ê –¥–ª—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è...")
        
        # 1. –ü–æ–ª—É—á–∞–µ–º –º–∞–∫—Å–∏–º—É–º —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        price_df = self.get_maximum_historical_data(years_back)
        if price_df.empty:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ü–µ–Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ")
            return pd.DataFrame()
        
        # 2. –ü–æ–ª—É—á–∞–µ–º –º–∞–∫—Å–∏–º—É–º –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        news_df = self.get_extended_news_data(years_back)
        
        # 3. –°–æ–∑–¥–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        enhanced_df = self._add_comprehensive_technical_indicators(price_df)
        
        # 4. –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤–æ—Å—Ç–Ω–æ–π sentiment
        if not news_df.empty:
            sentiment_df = self._aggregate_news_sentiment(news_df)
            final_df = enhanced_df.join(sentiment_df, how='left')
        else:
            logger.warning("‚ö†Ô∏è –ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –ø–æ–ª—É—á–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ü–µ–Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ")
            final_df = enhanced_df
            final_df['news_sentiment'] = 0.5  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π sentiment
        
        # 5. –§–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        final_df = self._finalize_master_dataset(final_df)
        
        logger.info(f"üéØ –ú–ê–°–¢–ï–†-–î–ê–¢–ê–°–ï–¢ –ì–û–¢–û–í:")
        logger.info(f"   üìä –ó–∞–ø–∏—Å–µ–π: {len(final_df):,}")
        logger.info(f"   üìà –ö–æ–ª–æ–Ω–æ–∫: {len(final_df.columns)}")
        logger.info(f"   ‚è∞ –ü–µ—Ä–∏–æ–¥: {final_df.index.min()} ‚Üí {final_df.index.max()}")
        logger.info(f"   üíæ –†–∞–∑–º–µ—Ä: {final_df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB")
        
        return final_df
    
    def _add_comprehensive_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        logger.info("üìà –†–∞—Å—á–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...")
        
        if 'GOLD_Close' not in df.columns:
            logger.error("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –∑–æ–ª–æ—Ç—É")
            return df
        
        gold = df['GOLD_Close']
        
        # –ë–∞–∑–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        df['RSI_14'] = self._calculate_rsi(gold, 14)
        df['RSI_21'] = self._calculate_rsi(gold, 21)
        
        # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
        for period in [5, 10, 20, 50, 200]:
            df[f'SMA_{period}'] = gold.rolling(window=period, min_periods=1).mean()
            df[f'EMA_{period}'] = gold.ewm(span=period, min_periods=1).mean()
        
        # MACD
        ema_12 = gold.ewm(span=12, min_periods=1).mean()
        ema_26 = gold.ewm(span=26, min_periods=1).mean()
        df['MACD'] = ema_12 - ema_26
        df['MACD_Signal'] = df['MACD'].ewm(span=9, min_periods=1).mean()
        df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']
        
        # Bollinger Bands
        for period in [20, 50]:
            sma = gold.rolling(window=period, min_periods=1).mean()
            std = gold.rolling(window=period, min_periods=1).std()
            df[f'BB_Upper_{period}'] = sma + (std * 2)
            df[f'BB_Lower_{period}'] = sma - (std * 2)
            df[f'BB_Width_{period}'] = df[f'BB_Upper_{period}'] - df[f'BB_Lower_{period}']
            df[f'BB_Position_{period}'] = (gold - df[f'BB_Lower_{period}']) / df[f'BB_Width_{period}']
        
        # –ò–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã
        for period in [1, 4, 24, 168]:  # 1—á, 4—á, 1–¥, 1–Ω–µ–¥
            df[f'Price_Change_{period}h'] = gold.pct_change(period)
            df[f'Price_Return_{period}h'] = np.log(gold / gold.shift(period))
        
        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        for period in [24, 168]:  # 1–¥, 1–Ω–µ–¥
            df[f'Volatility_{period}h'] = gold.rolling(window=period, min_periods=1).std()
        
        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å –¥—Ä—É–≥–∏–º–∏ –∞–∫—Ç–∏–≤–∞–º–∏
        if 'DXY_Close' in df.columns:
            for period in [24, 168]:
                df[f'GOLD_DXY_Corr_{period}h'] = gold.rolling(period, min_periods=1).corr(df['DXY_Close'])
        
        if 'VIX_Close' in df.columns:
            for period in [24, 168]:
                df[f'GOLD_VIX_Corr_{period}h'] = gold.rolling(period, min_periods=1).corr(df['VIX_Close'])
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['Hour'] = df.index.hour
        df['DayOfWeek'] = df.index.dayofweek
        df['Month'] = df.index.month
        df['Quarter'] = df.index.quarter
        
        # –õ–∞–≥–∏
        for lag in [1, 2, 4, 8, 24]:
            df[f'GOLD_Lag_{lag}'] = gold.shift(lag)
            df[f'RSI_Lag_{lag}'] = df['RSI_14'].shift(lag)
        
        logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len([col for col in df.columns if col not in ['GOLD_Close']])} —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤")
        return df
    
    def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> pd.Series:
        """–†–∞—Å—á–µ—Ç RSI"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()
        rs = gain / loss.replace(0, np.nan)
        rsi = 100 - (100 / (1 + rs))
        return rsi.fillna(50)
    
    def _aggregate_news_sentiment(self, news_df: pd.DataFrame) -> pd.DataFrame:
        """–ê–≥—Ä–µ–≥–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ sentiment –ø–æ –≤—Ä–µ–º–µ–Ω–∏"""
        logger.info("üì∞ –ê–≥—Ä–µ–≥–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ sentiment...")
        
        # –†–µ—Å–µ–º–ø–ª–∏—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ —á–∞—Å–∞–º
        hourly_sentiment = news_df.resample('h').agg({
            'sentiment_score': ['mean', 'std', 'count'],
        }).fillna(0)
        
        # –£–ø—Ä–æ—â–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏
        hourly_sentiment.columns = ['news_sentiment_mean', 'news_sentiment_std', 'news_count']
        
        # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ –¥–ª—è sentiment
        for period in [6, 24, 168]:  # 6—á, 1–¥, 1–Ω–µ–¥
            hourly_sentiment[f'news_sentiment_{period}h'] = (
                hourly_sentiment['news_sentiment_mean']
                .rolling(window=period, min_periods=1)
                .mean()
            )
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
        hourly_sentiment = hourly_sentiment.fillna(method='ffill').fillna(0)
        
        logger.info(f"‚úÖ –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–æ sentiment –¥–ª—è {len(hourly_sentiment)} —á–∞—Å–æ–≤")
        return hourly_sentiment
    
    def _finalize_master_dataset(self, df: pd.DataFrame) -> pd.DataFrame:
        """–§–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        logger.info("üîß –§–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        
        # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ –æ—Å–Ω–æ–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        df = df.dropna(subset=['GOLD_Close'])
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN
        df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)
        
        # –£–±–∏—Ä–∞–µ–º –≤—ã–±—Ä–æ—Å—ã (3 sigma)
        for col in df.select_dtypes(include=[np.number]).columns:
            if 'GOLD' in col and 'Close' in col:
                mean = df[col].mean()
                std = df[col].std()
                df = df[(df[col] >= mean - 3*std) & (df[col] <= mean + 3*std)]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        df.sort_index(inplace=True)
        
        logger.info(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {len(df)} –∑–∞–ø–∏—Å–µ–π, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
        return df
    
    def save_master_dataset(self, df: pd.DataFrame, filename: str = None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–∞—Å—Ç–µ—Ä-–¥–∞—Ç–∞—Å–µ—Ç–∞"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"master_training_dataset_{timestamp}"
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
        csv_path = f"{filename}.csv"
        pickle_path = f"{filename}.pkl"
        
        # CSV –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
        df.to_csv(csv_path)
        logger.info(f"üíæ CSV —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {csv_path}")
        
        # Pickle –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
        with open(pickle_path, 'wb') as f:
            pickle.dump(df, f)
        logger.info(f"üíæ Pickle —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {pickle_path}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = {
            'rows': len(df),
            'columns': len(df.columns),
            'start_date': str(df.index.min()),
            'end_date': str(df.index.max()),
            'memory_mb': df.memory_usage(deep=True).sum() / 1024 / 1024,
            'column_list': list(df.columns)
        }
        
        stats_path = f"{filename}_stats.json"
        import json
        with open(stats_path, 'w') as f:
            json.dump(stats, f, indent=2)
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {stats_path}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–∞—Å—Ç–µ—Ä-–¥–∞—Ç–∞—Å–µ—Ç–∞"""
    pipeline = ExtendedDataPipeline()
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∑–∞ 3 –≥–æ–¥–∞
    master_df = pipeline.create_master_dataset(years_back=3)
    
    if not master_df.empty:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        pipeline.save_master_dataset(master_df)
        
        print("\n" + "="*60)
        print("üéØ –ú–ê–°–¢–ï–†-–î–ê–¢–ê–°–ï–¢ –î–õ–Ø –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø –ì–û–¢–û–í!")
        print("="*60)
        print(f"üìä –ó–∞–ø–∏—Å–µ–π: {len(master_df):,}")
        print(f"üìà –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(master_df.columns)}")
        print(f"‚è∞ –ü–µ—Ä–∏–æ–¥: {master_df.index.min()} ‚Üí {master_df.index.max()}")
        print("\nüìã –¢–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–∏:")
        for i, col in enumerate(master_df.columns[:10], 1):
            print(f"  {i}. {col}")
        if len(master_df.columns) > 10:
            print(f"  ... –∏ –µ—â–µ {len(master_df.columns) - 10} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        print("\n‚úÖ –ì–æ—Ç–æ–≤ –¥–ª—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è RL –º–æ–¥–µ–ª–∏!")
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç")


if __name__ == "__main__":
    main()