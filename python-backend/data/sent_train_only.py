#!/usr/bin/env python3
"""
SENTIMENT-ONLY RL Trading Agent
–¢–æ—Ä–≥—É–µ—Ç –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ sentiment –∞–Ω–∞–ª–∏–∑–∞ –±–µ–∑ –¥—Ä—É–≥–∏—Ö —Ñ–∏—á–µ–π
–¶–µ–ª—å: –∏–∑–æ–ª–∏—Ä–æ–≤–∞—Ç—å –≤–ª–∏—è–Ω–∏–µ sentiment –Ω–∞ —Ç–æ—Ä–≥–æ–≤—ã–µ —Ä–µ—à–µ–Ω–∏—è
"""
import gymnasium as gym
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import logging
from datetime import datetime
from mathematical_sentiment_model import MathematicalSentimentAnalyzer, get_enhanced_sentiment
from typing import Dict, Tuple

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SentimentOnlyTradingEnvironment(gym.Env):
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Å—Ä–µ–¥–∞ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ –¢–û–õ–¨–ö–û –Ω–∞ sentiment
    –§–∏—á–∏: —Ç–æ–ª—å–∫–æ sentiment + –±–∞–∑–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Ä—Ç—Ñ–µ–ª—è
    """
    
    def __init__(self, data, initial_balance=10000, commission=0.001, lookback_window=12):
        super().__init__()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        required_columns = ['GOLD_Close', 'sentiment']
        missing_columns = [col for col in required_columns if col not in data.columns]
        if missing_columns:
            raise ValueError(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
        
        self.data = data.copy()
        self.initial_balance = initial_balance
        self.commission = commission
        self.lookback_window = lookback_window
        
        # üî• –ù–û–í–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ –î–õ–Ø SL –ò –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–• –ü–û–ó–ò–¶–ò–ô
        self.stop_loss_pct = 0.04  # 2% Stop Loss
        self.max_position_size = 0.6  # –ú–∞–∫—Å–∏–º—É–º 90% –æ—Ç –±–∞–ª–∞–Ω—Å–∞
        self.min_position_size = 0.1  # –ú–∏–Ω–∏–º—É–º 10% –æ—Ç –±–∞–ª–∞–Ω—Å–∞
        
        # –¢–û–õ–¨–ö–û sentiment —Ñ–∏—á–∏!
        self.feature_columns = ['sentiment']
        
        logger.info(f"üìä Sentiment-Only —Ñ–∏—á–∏: {self.feature_columns}")
        
        # –°–æ–∑–¥–∞–µ–º sentiment –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        self.sentiment_weights = {
            'marketaux_news': 0.45,      # MarketAux
            'fear_greed_index': 0.25,    # Fear & Greed  
            'golden_cross_trend': 0.13,  # Golden Cross
           # 'vix_momentum': 0.07,        # VIX
            #'price_momentum': 0.10       # Price momentum
        }
        
        self.sentiment_reliabilities = {
            'marketaux_news': 0.83,
            'fear_greed_index': 0.86,
            'golden_cross_trend': 0.54,
            #'vix_momentum': 0.69,
            #'price_momentum': 0.51
        }
        
        self.sentiment_analyzer = MathematicalSentimentAnalyzer(
            custom_weights=self.sentiment_weights,
            custom_reliabilities=self.sentiment_reliabilities
        )
        
        logger.info("üß† Sentiment –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å–æ–∑–¥–∞–Ω —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        self._prepare_data()
        
        # Gym spaces
        self.action_space = gym.spaces.Discrete(3)  # 0: Hold, 1: Buy, 2: Sell
        
        # Observation: sentiment lookback + –ø–æ—Ä—Ç—Ñ–µ–ª—å (5 –∑–Ω–∞—á–µ–Ω–∏–π)
        observation_space_size = lookback_window + 5  # sentiment history + portfolio state + SL info
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf, shape=(observation_space_size,), dtype=np.float32
        )
        
        logger.info(f"üéØ Sentiment-Only Environment:")
        logger.info(f"   üìä –î–∞–Ω–Ω—ã—Ö: {len(self.data)} –∑–∞–ø–∏—Å–µ–π")
        logger.info(f"   üß† Observation space: {observation_space_size}")
        logger.info(f"   üìà –¢–æ–ª—å–∫–æ sentiment + –ø–æ—Ä—Ç—Ñ–µ–ª—å")

    def _prepare_data(self):
        """–ü—Ä–æ—Å—Ç–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ sentiment –¥–∞–Ω–Ω—ã—Ö...")
        
        # –£–±–∏—Ä–∞–µ–º NaN
        initial_length = len(self.data)
        self.data = self.data.dropna(subset=['GOLD_Close', 'sentiment'])
        final_length = len(self.data)
        
        if final_length < initial_length:
            logger.warning(f"‚ö†Ô∏è –£–¥–∞–ª–µ–Ω–æ {initial_length - final_length} —Å—Ç—Ä–æ–∫ —Å NaN")
        
        # –ù–ï –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º sentiment - –æ–Ω —É–∂–µ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [-1, 1]
        logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã: {len(self.data)} –∑–∞–ø–∏—Å–µ–π")

    def reset(self, seed=None):
        """–°–±—Ä–æ—Å —Å—Ä–µ–¥—ã"""
        super().reset(seed=seed)
        self.current_step = self.lookback_window
        self.balance = self.initial_balance
        self.position = 0  # 0: –Ω–µ—Ç –ø–æ–∑–∏—Ü–∏–∏, 1: –ª–æ–Ω–≥
        self.position_size = 0.0
        self.entry_price = 0.0
        self.stop_loss_price = 0.0  # üî• Stop Loss —Ü–µ–Ω–∞
        self.trade_count = 0
        self.total_profit = 0.0
        self.sl_triggered_count = 0  # üî• –°—á–µ—Ç—á–∏–∫ —Å—Ä–∞–±–æ—Ç–∞–≤—à–∏—Ö SL
        
        return self._get_observation(), {}

    def _get_observation(self):
        """
        –ü–æ–ª—É—á–∞–µ—Ç –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ: sentiment history + —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Ä—Ç—Ñ–µ–ª—è
        """
        # 1. Sentiment history –∑–∞ lookback_window
        start = max(0, self.current_step - self.lookback_window)
        end = self.current_step
        
        sentiment_history = []
        
        # –°–æ–±–∏—Ä–∞–µ–º sentiment –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ lookback_window —à–∞–≥–æ–≤
        for i in range(start, end):
            if i < len(self.data):
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º enhanced sentiment
                    data_row = self.data.iloc[i]
                    enhanced_sentiment, confidence = get_enhanced_sentiment(
                        self.sentiment_analyzer, data_row
                    )
                    
                    # –ï—Å–ª–∏ confidence –≤—ã—Å–æ–∫–∏–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º enhanced, –∏–Ω–∞—á–µ –±–∞–∑–æ–≤—ã–π
                    if confidence > 0.4:
                        sentiment_history.append(enhanced_sentiment)
                    else:
                        sentiment_history.append(data_row['sentiment'])
                        
                except Exception as e:
                    # Fallback –∫ –±–∞–∑–æ–≤–æ–º—É sentiment
                    sentiment_history.append(self.data.iloc[i]['sentiment'])
            else:
                sentiment_history.append(0.0)  # Padding
        
        # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–æ –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        while len(sentiment_history) < self.lookback_window:
            sentiment_history.insert(0, 0.0)
        
        # 2. –°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Ä—Ç—Ñ–µ–ª—è —Å SL –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        current_price = self.data['GOLD_Close'].iloc[self.current_step]
        current_equity = self.balance
        if self.position == 1:
            current_equity += self.position_size * current_price
        
        unrealized_pnl = 0
        sl_distance = 0  # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ Stop Loss
        
        if self.position == 1 and self.entry_price > 0:
            unrealized_pnl = (current_price - self.entry_price) / self.entry_price
            if self.stop_loss_price > 0:
                sl_distance = (current_price - self.stop_loss_price) / current_price
        
        portfolio_state = [
            self.balance / self.initial_balance,  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –±–∞–ª–∞–Ω—Å
            float(self.position),                 # –ü–æ–∑–∏—Ü–∏—è (0 –∏–ª–∏ 1)
            self.position_size / max(1.0, self.balance / current_price) if current_price > 0 else 0,
            unrealized_pnl,                      # –ù–µ—Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π P&L
            sl_distance                          # üî• –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ SL
        ]
        
        # 3. –û–±—ä–µ–¥–∏–Ω—è–µ–º
        observation = sentiment_history + portfolio_state
        observation = np.array(observation, dtype=np.float32)
        
        # –ó–∞—â–∏—Ç–∞ –æ—Ç NaN
        if np.any(np.isnan(observation)) or np.any(np.isinf(observation)):
            observation = np.nan_to_num(observation, nan=0.0, posinf=1.0, neginf=-1.0)
        
        return observation

    def step(self, action):
        """
        –®–∞–≥ –≤ —Å—Ä–µ–¥–µ —Å STOP LOSS –∏ –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ú–ò –ü–û–ó–ò–¶–ò–Ø–ú–ò
        """
        current_price = self.data['GOLD_Close'].iloc[self.current_step]
        reward = 0
        sl_triggered = False
        
        # üî• –ü–†–û–í–ï–†–ö–ê STOP LOSS –í –ü–ï–†–í–£–Æ –û–ß–ï–†–ï–î–¨
        if self.position == 1 and self.stop_loss_price > 0:
            if current_price <= self.stop_loss_price:
                # Stop Loss —Å—Ä–∞–±–æ—Ç–∞–ª!
                sl_triggered = True
                
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ –ø–æ–∑–∏—Ü–∏–∏
                commission_cost = current_price * self.position_size * self.commission
                self.balance += current_price * self.position_size - commission_cost
                
                # –£–±—ã—Ç–æ–∫ –æ—Ç SL
                loss = (current_price - self.entry_price) * self.position_size - commission_cost
                self.total_profit += loss
                
                # –®—Ç—Ä–∞—Ñ –∑–∞ —Å—Ä–∞–±–æ—Ç–∞–≤—à–∏–π SL
                reward -= 10  # –®—Ç—Ä–∞—Ñ –∑–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–∞–º–∏
                
                self.position = 0
                self.position_size = 0
                self.entry_price = 0
                self.stop_loss_price = 0
                self.sl_triggered_count += 1
                
                logger.debug(f"üõë Stop Loss triggered at {current_price:.2f}, Loss: ${loss:.2f}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π sentiment –¥–ª—è —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ä–µ—à–µ–Ω–∏–π
        try:
            current_data_row = self.data.iloc[self.current_step]
            enhanced_sentiment, confidence = get_enhanced_sentiment(
                self.sentiment_analyzer, current_data_row
            )
            
            if confidence > 0.4:
                current_sentiment = enhanced_sentiment
            else:
                current_sentiment = current_data_row['sentiment']
                
        except Exception:
            current_sentiment = self.data.iloc[self.current_step]['sentiment']
        
        # –¢–û–†–ì–û–í–ê–Ø –õ–û–ì–ò–ö–ê (–µ—Å–ª–∏ SL –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª)
        if not sl_triggered:
            
            if action == 1:  # Buy
                if self.position == 0:  # –û—Ç–∫—Ä—ã–≤–∞–µ–º –ª–æ–Ω–≥ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç –ø–æ–∑–∏—Ü–∏–∏
                    
                    # üî• –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô —Ä–∞–∑–º–µ—Ä –ø–æ–∑–∏—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ sentiment
                    sentiment_strength = abs(current_sentiment)
                    position_multiplier = self.min_position_size + (sentiment_strength * (self.max_position_size - self.min_position_size))
                    position_multiplier = np.clip(position_multiplier, self.min_position_size, self.max_position_size)
                    
                    self.position = 1
                    self.position_size = (self.balance * position_multiplier) / current_price
                    self.entry_price = current_price
                    
                    # üî• –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Stop Loss
                    self.stop_loss_price = current_price * (1 - self.stop_loss_pct)
                    
                    self.balance -= self.position_size * current_price * (1 + self.commission)
                    self.trade_count += 1
                    
                    # –ù–∞–≥—Ä–∞–¥–∞ –∑–∞ –ø–æ–∫—É–ø–∫—É –ø—Ä–∏ –ø–æ–∑–∏—Ç–∏–≤–Ω–æ–º sentiment
                    if current_sentiment > 0.1:
                        reward += current_sentiment * 3 + sentiment_strength * 2  # –ë–æ–Ω—É—Å –∑–∞ —Å–∏–ª—É —Å–∏–≥–Ω–∞–ª–∞
                    else:
                        reward -= 1  # –ú—è–≥–∫–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–∫—É–ø–∫—É –≤ –Ω–µ–≥–∞—Ç–∏–≤–µ
                    
                    logger.debug(f"üìà BUY: Size={position_multiplier:.1%}, SL={self.stop_loss_price:.2f}")
                        
            elif action == 2:  # Sell
                if self.position == 1:  # –ó–∞–∫—Ä—ã–≤–∞–µ–º –ª–æ–Ω–≥
                    commission_cost = current_price * self.position_size * self.commission
                    self.balance += current_price * self.position_size - commission_cost
                    
                    # –ü—Ä–∏–±—ã–ª—å/—É–±—ã—Ç–æ–∫
                    profit = (current_price - self.entry_price) * self.position_size - commission_cost
                    self.total_profit += profit
                    
                    # –ù–∞–≥—Ä–∞–¥–∞ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–∞ –ø—Ä–∏–±—ã–ª–∏
                    profit_ratio = profit / (self.entry_price * self.position_size)
                    reward += profit_ratio * 50  # –û—Å–Ω–æ–≤–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞
                    
                    # –ë–æ–Ω—É—Å –∑–∞ –ø—Ä–æ–¥–∞–∂—É –ø—Ä–∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–º sentiment
                    if current_sentiment < -0.1:
                        reward += abs(current_sentiment) * 2
                    
                    # –ë–æ–Ω—É—Å –∑–∞ –∏–∑–±–µ–∂–∞–Ω–∏–µ SL
                    if profit > 0:
                        reward += 2  # –ë–æ–Ω—É—Å –∑–∞ –ø—Ä–∏–±—ã–ª—å–Ω—É—é —Å–¥–µ–ª–∫—É
                    
                    self.position = 0
                    self.position_size = 0
                    self.entry_price = 0
                    self.stop_loss_price = 0
                    
                    logger.debug(f"üìâ SELL: Profit=${profit:.2f}, P&L={profit_ratio:.1%}")
            
            # Hold (action == 0) - –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —à—Ç—Ä–∞—Ñ
            elif action == 0:
                if abs(current_sentiment) < 0.05:
                    reward += 0.1  # –ë–æ–Ω—É—Å –∑–∞ hold –ø—Ä–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
                else:
                    reward -= 0.05  # –ú—è–≥–∫–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ —É–ø—É—â–µ–Ω–Ω—É—é –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ rewards
        
        # 1. Reward –∑–∞ —É–¥–µ—Ä–∂–∞–Ω–∏–µ –ø—Ä–∏–±—ã–ª—å–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏
        if self.position == 1 and not sl_triggered:
            unrealized_pnl = (current_price - self.entry_price) / self.entry_price
            
            if unrealized_pnl > 0:
                # –ë–æ–Ω—É—Å –∑–∞ —É–¥–µ—Ä–∂–∞–Ω–∏–µ –ø—Ä–∏–±—ã–ª—å–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏ –ø—Ä–∏ –ø–æ–∑–∏—Ç–∏–≤–Ω–æ–º sentiment
                if current_sentiment > 0:
                    reward += unrealized_pnl * current_sentiment * 5
            else:
                # –®—Ç—Ä–∞—Ñ –∑–∞ —É–¥–µ—Ä–∂–∞–Ω–∏–µ —É–±—ã—Ç–æ—á–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏ –ø—Ä–∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–º sentiment
                if current_sentiment < 0:
                    reward += unrealized_pnl * abs(current_sentiment) * 3  # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π reward
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ—Å–∞–¥–∫–∏
        current_equity = self.balance
        if self.position == 1:
            current_equity += self.position_size * current_price
        
        equity_ratio = current_equity / self.initial_balance
        
        if equity_ratio < 0.7:  # –ü—Ä–æ—Å–∞–¥–∫–∞ > 30%
            reward -= 20
            done = True  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
        else:
            done = False
        
        self.current_step += 1
        
        # –û–±—ã—á–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        if self.current_step >= len(self.data) - 1:
            done = True
        
        return self._get_observation(), reward, done, False, {
            'sl_triggered': sl_triggered,
            'current_sentiment': current_sentiment
        }

    def get_portfolio_stats(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ—Ä—Ç—Ñ–µ–ª—è —Å SL –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
        current_price = self.data['GOLD_Close'].iloc[self.current_step - 1] if self.current_step > 0 else self.data['GOLD_Close'].iloc[0]
        current_equity = self.balance
        
        if self.position == 1:
            current_equity += self.position_size * current_price
        
        total_return = (current_equity - self.initial_balance) / self.initial_balance
        
        return {
            'balance': self.balance,
            'position': self.position,
            'current_equity': current_equity,
            'total_return': total_return,
            'trade_count': self.trade_count,
            'total_profit': self.total_profit,
            'sl_triggered_count': self.sl_triggered_count,  # üî• –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ä–∞–±–æ—Ç–∞–≤—à–∏—Ö SL
            'sl_rate': self.sl_triggered_count / max(1, self.trade_count) * 100  # üî• –ü—Ä–æ—Ü–µ–Ω—Ç SL
        }

# –ü–†–û–°–¢–û–ô PPO –ê–ì–ï–ù–¢ (–∏–∑ –ª—É—á—à–µ–≥–æ)
class SimpleActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(SimpleActorCritic, self).__init__()
        
        self.actor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Softmax(dim=-1)
        )
        
        self.critic = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, state):
        return self.actor(state), self.critic(state)

class SimpleMemory:
    def __init__(self):
        self.actions = []
        self.states = []
        self.logprobs = []
        self.rewards = []
        self.is_terminals = []
    
    def clear(self):
        del self.actions[:]
        del self.states[:]
        del self.logprobs[:]
        del self.rewards[:]
        del self.is_terminals[:]

class SentimentPPOAgent:
    def __init__(self, state_dim, action_dim, lr=1e-4, gamma=0.99, hidden_dim=128):
        self.gamma = gamma
        self.eps_clip = 0.2
        self.K_epochs = 4
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        self.policy = SimpleActorCritic(state_dim, action_dim, hidden_dim).to(self.device)
        self.policy_old = SimpleActorCritic(state_dim, action_dim, hidden_dim).to(self.device)
        
        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)
        self.policy_old.load_state_dict(self.policy.state_dict())
        self.MseLoss = nn.MSELoss()
        
        logger.info(f"ü§ñ Sentiment PPO Agent: LR={lr}, Hidden={hidden_dim}, Device={self.device}")
    
    def select_action(self, state, memory):
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).to(self.device).unsqueeze(0)
            action_probs, _ = self.policy_old(state_tensor)
            dist = Categorical(action_probs)
            action = dist.sample()
            
            memory.states.append(state_tensor)
            memory.actions.append(action)
            memory.logprobs.append(dist.log_prob(action))
            
        return action.item()
    
    def update(self, memory):
        if len(memory.rewards) <= 1:
            return 0.0
        
        # Discounted rewards
        rewards = []
        discounted_reward = 0
        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):
            if is_terminal:
                discounted_reward = 0
            discounted_reward = reward + (self.gamma * discounted_reward)
            rewards.insert(0, discounted_reward)
        
        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)
        
        old_states = torch.squeeze(torch.stack(memory.states, dim=0)).to(self.device).detach()
        old_actions = torch.squeeze(torch.stack(memory.actions, dim=0)).to(self.device).detach()
        old_logprobs = torch.squeeze(torch.stack(memory.logprobs, dim=0)).to(self.device).detach()
        
        # PPO update
        for _ in range(self.K_epochs):
            action_probs, state_values = self.policy(old_states)
            state_values = torch.squeeze(state_values)
            
            dist = Categorical(action_probs)
            logprobs = dist.log_prob(old_actions)
            dist_entropy = dist.entropy()
            
            ratios = torch.exp(logprobs - old_logprobs.detach())
            advantages = rewards - state_values.detach()
            
            surr1 = ratios * advantages
            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages
            
            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy
            
            self.optimizer.zero_grad()
            loss.mean().backward()
            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), max_norm=0.5)
            self.optimizer.step()
        
        self.policy_old.load_state_dict(self.policy.state_dict())
        return loss.mean().item()

def main():
    logger.info("üöÄ SENTIMENT-ONLY RL TRADING AGENT")
    logger.info("üéØ –¢–æ—Ä–≥—É–µ—Ç –¢–û–õ–¨–ö–û –Ω–∞ sentiment, –∏–≥–Ω–æ—Ä–∏—Ä—É—è –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏")
    logger.info("=" * 60)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    try:
        possible_files = [
            'master_training_dataset.csv',
            'gold_data_with_sentiment_hourly.csv',
            'automated_gold_data_with_sentiment.csv'
        ]
        
        data_df = None
        for filename in possible_files:
            try:
                data_df = pd.read_csv(filename, index_col=0, parse_dates=True)
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω: {filename}")
                break
            except FileNotFoundError:
                continue
        
        if data_df is None:
            raise FileNotFoundError("‚ùå –î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        logger.info(f"üìä –î–∞–Ω–Ω—ã–µ: {data_df.shape}")
        logger.info(f"üìÖ –ü–µ—Ä–∏–æ–¥: {data_df.index.min()} ‚Üí {data_df.index.max()}")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        return
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    train_size = int(len(data_df) * 0.8)
    train_data = data_df.iloc[:train_size].copy()
    test_data = data_df.iloc[train_size:].copy()
    
    logger.info(f"üîÑ Train: {len(train_data)}, Test: {len(test_data)}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã
    try:
        env = SentimentOnlyTradingEnvironment(train_data, lookback_window=12)
        state_dim = env.observation_space.shape[0]
        action_dim = env.action_space.n
        
        logger.info(f"üéØ –°—Ä–µ–¥–∞ —Å–æ–∑–¥–∞–Ω–∞: State={state_dim}, Actions={action_dim}")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ä–µ–¥—ã: {e}")
        return
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
    agent = SentimentPPOAgent(state_dim, action_dim, lr=2e-4, hidden_dim=128)
    memory = SimpleMemory()
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    num_episodes = 150
    update_timestep = 500
    
    logger.info(f"üéì –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
    logger.info(f"   Episodes: {num_episodes}")
    logger.info(f"   Update frequency: {update_timestep}")
    logger.info("=" * 60)
    
    # –û–±—É—á–µ–Ω–∏–µ
    timestep = 0
    best_return = -float('inf')
    
    for episode in range(1, num_episodes + 1):
        state, _ = env.reset()
        episode_reward = 0
        
        try:
            while True:
                timestep += 1
                action = agent.select_action(state, memory)
                state, reward, done, _, _ = env.step(action)
                
                memory.rewards.append(reward)
                memory.is_terminals.append(done)
                episode_reward += reward
                
                if timestep % update_timestep == 0:
                    loss = agent.update(memory)
                    memory.clear()
                
                if done:
                    break
        
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —ç–ø–∏–∑–æ–¥–µ {episode}: {e}")
            break
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = env.get_portfolio_stats()
        
        if episode % 10 == 0:
            logger.info(f"Episode {episode:3d} | "
                       f"Reward: {episode_reward:8.2f} | "
                       f"Return: {stats['total_return']*100:6.2f}% | "
                       f"Trades: {stats['trade_count']:3d} | "
                       f"SL Rate: {stats['sl_rate']:4.1f}% | "
                       f"Profit: ${stats['total_profit']:8.2f}")
        
        # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ë–ï–ó —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        if stats['total_return'] > best_return:
            best_return = stats['total_return']
            # –£–±—Ä–∞–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —á–∏—Å—Ç–æ—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    logger.info("\n" + "="*60)
    logger.info("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –û–¢–õ–û–ñ–ï–ù–ù–´–• –î–ê–ù–ù–´–•")
    logger.info("="*60)
    
    try:
        test_env = SentimentOnlyTradingEnvironment(test_data, lookback_window=12)
        test_state, _ = test_env.reset()
        
        steps = 0
        max_steps = len(test_data) - 15
        
        while steps < max_steps:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(test_state).to(agent.device).unsqueeze(0)
                action_probs, _ = agent.policy_old(state_tensor)
                action = torch.argmax(action_probs).item()
            
            test_state, _, done, _, _ = test_env.step(action)
            steps += 1
            
            if done:
                break
        
        test_stats = test_env.get_portfolio_stats()
        
        print(f"\nüèÜ –†–ï–ó–£–õ–¨–¢–ê–¢–´ SENTIMENT + SL + DYNAMIC TRADING:")
        print(f"üìà Test Return:     {test_stats['total_return']*100:8.2f}%")
        print(f"üí∞ Final Equity:    ${test_stats['current_equity']:8.2f}")
        print(f"üîÑ Total Trades:    {test_stats['trade_count']:8d}")
        print(f"üíµ Total Profit:    ${test_stats['total_profit']:8.2f}")
        print(f"üõë SL Triggered:    {test_stats['sl_triggered_count']:8d} ({test_stats['sl_rate']:.1f}%)")
        print("="*60)
        
        if test_stats['total_return'] > 0.05:
            print("‚úÖ –£–°–ü–ï–•! Sentiment —Ç–æ—Ä–≥–æ–≤–ª—è –ø—Ä–∏–±—ã–ª—å–Ω–∞!")
        else:
            print("‚ö†Ô∏è Sentiment —Ç–æ—Ä–≥–æ–≤–ª—è —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏")
    
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
    
    logger.info("‚úÖ Sentiment-Only –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

if __name__ == '__main__':
    main()