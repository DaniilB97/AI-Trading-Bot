#!/usr/bin/env python3
"""
ÐŸÐžÐ›ÐÐÐ¯ Optuna Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ: RL Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ + Sentiment Ð²ÐµÑÐ° + Reliability
"""
import optuna
import torch
import pandas as pd
import logging
from updated_train_rl_model import *

def objective(trial):
    """ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð’Ð¡Ð•Ð¥ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²"""
    
    # 1. RL ÐŸÐÐ ÐÐœÐ•Ð¢Ð Ð«
    lr = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)
    gamma = trial.suggest_float('gamma', 0.95, 0.999)
    hidden_dim = trial.suggest_categorical('hidden_dim', [128, 256, 512])
    lookback_window = trial.suggest_categorical('lookback_window', [12, 24, 48])
    
    # 2. SENTIMENT Ð’Ð•Ð¡Ð (Ð´Ð¾Ð»Ð¶Ð½Ñ‹ ÑÑƒÐ¼Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒÑÑ Ðº 1.0)
    marketaux_weight = trial.suggest_float('marketaux_weight', 0.15, 0.50)
    fear_greed_weight = trial.suggest_float('fear_greed_weight', 0.10, 0.40)
    golden_cross_weight = trial.suggest_float('golden_cross_weight', 0.05, 0.35)
    vix_weight = trial.suggest_float('vix_weight', 0.05, 0.20)
    # Price weight = Ð¾ÑÑ‚Ð°Ð²ÑˆÐµÐµÑÑ Ð´Ð¾ 1.0
    
    # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð²ÐµÑÐ¾Ð²
    total_weight = marketaux_weight + fear_greed_weight + golden_cross_weight + vix_weight
    remaining_weight = max(0.05, 1.0 - total_weight)  # ÐœÐ¸Ð½Ð¸Ð¼ÑƒÐ¼ 5% Ð´Ð»Ñ price
    
    sentiment_weights = {
        'marketaux_news': marketaux_weight / (total_weight + remaining_weight),
        'fear_greed_index': fear_greed_weight / (total_weight + remaining_weight),
        'golden_cross_trend': golden_cross_weight / (total_weight + remaining_weight),
        'vix_momentum': vix_weight / (total_weight + remaining_weight),
        'price_momentum': remaining_weight / (total_weight + remaining_weight)
    }
    
    # 3. SENTIMENT RELIABILITY
    sentiment_reliabilities = {
        'marketaux_news': trial.suggest_float('marketaux_reliability', 0.6, 0.95),
        'fear_greed_index': trial.suggest_float('fear_greed_reliability', 0.7, 0.99),
        'golden_cross_trend': trial.suggest_float('golden_cross_reliability', 0.5, 0.85),
        'vix_momentum': trial.suggest_float('vix_reliability', 0.4, 0.75),
        'price_momentum': trial.suggest_float('price_reliability', 0.3, 0.70)
    }
    
    # 4. REWARD ÐŸÐÐ ÐÐœÐ•Ð¢Ð Ð«
    golden_cross_reward_bonus = trial.suggest_float('gc_reward_bonus', 1.0, 5.0)
    drawdown_penalty = trial.suggest_float('drawdown_penalty', 5.0, 30.0)
    
    try:
        # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…
        data_df = pd.read_csv('master_training_dataset.csv', index_col=0, parse_dates=True)
        train_data, val_data, test_data = split_data_by_time(data_df, 0.6, 0.2)
        
        # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÑ€ÐµÐ´Ñ‹ Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼Ñ‹Ð¼Ð¸ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°Ð¼Ð¸
        env = create_optimized_environment(
            train_data, lookback_window, sentiment_weights, sentiment_reliabilities,
            golden_cross_reward_bonus, drawdown_penalty
        )
        
        agent = PPOAgent(env.observation_space.shape[0], env.action_space.n, 
                        lr=lr, gamma=gamma, hidden_dim=hidden_dim)
        
        # Ð‘Ñ‹ÑÑ‚Ñ€Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ optuna
        episodes = trial.suggest_categorical('episodes', [30, 50, 80])
        val_performance = train_and_validate(env, agent, train_data, val_data, episodes)
        
        return val_performance
        
    except Exception as e:
        logging.error(f"Trial failed: {e}")
        return -1.0

def create_optimized_environment(data, lookback_window, sentiment_weights, 
                               sentiment_reliabilities, gc_bonus, dd_penalty):
    """Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÑ€ÐµÐ´Ñ‹ Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¼Ð¸ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°Ð¼Ð¸"""
    
    # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÐºÐ°ÑÑ‚Ð¾Ð¼Ð½ÑƒÑŽ ÑÑ€ÐµÐ´Ñƒ
    class OptimizedEnvironment(EnhancedGoldTradingEnvironment):
        def __init__(self, data, lookback_window):
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð´Ð»Ñ sentiment Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð°
            self.sentiment_weights = sentiment_weights
            self.sentiment_reliabilities = sentiment_reliabilities
            self.gc_bonus = gc_bonus
            self.dd_penalty = dd_penalty
            
            super().__init__(data, lookback_window=lookback_window)
        
        def step(self, action):
            # Ð’Ñ‹Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ step
            state, reward, done, truncated, info = super().step(action)
            
            # ÐœÐ¾Ð´Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€ÑƒÐµÐ¼ reward Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¼Ð¸ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°Ð¼Ð¸
            current_price = self.data['GOLD_Close'].iloc[self.current_step]
            
            # Golden Cross Ð±Ð¾Ð½ÑƒÑ (Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼Ñ‹Ð¹)
            if 'golden_cross_trend' in self.data.columns:
                gc_trend = self.data['golden_cross_trend'].iloc[self.current_step]
                if action == 1 and gc_trend > 0:
                    reward += self.gc_bonus
                elif action == 1 and gc_trend < 0:
                    reward -= self.gc_bonus
            
            # Ð¨Ñ‚Ñ€Ð°Ñ„ Ð·Ð° Ð¿Ñ€Ð¾ÑÐ°Ð´ÐºÐ¸ (Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼Ñ‹Ð¹)
            current_equity = self.balance
            if self.position == 1:
                current_equity += self.position_size * current_price
            
            equity_ratio = current_equity / self.initial_balance
            if equity_ratio < 0.8:
                reward -= self.dd_penalty
            
            return state, reward, done, truncated, info
    
    return OptimizedEnvironment(data, lookback_window)

def train_and_validate(env, agent, train_data, val_data, episodes):
    """Ð‘Ñ‹ÑÑ‚Ñ€Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¸ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ"""
    memory = Memory()
    
    # ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ
    for episode in range(episodes):
        state, _ = env.reset()
        
        for step in range(800):  # ÐšÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ðµ ÑÐ¿Ð¸Ð·Ð¾Ð´Ñ‹ Ð´Ð»Ñ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚Ð¸
            action = agent.select_action(state, memory)
            state, reward, done, _, _ = env.step(action)
            memory.rewards.append(reward)
            memory.is_terminals.append(done)
            
            if step % 100 == 0:
                agent.update(memory)
                memory.clear()
            
            if done:
                break
    
    # Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ
    val_env = create_optimized_environment(
        val_data, env.lookback_window, env.sentiment_weights, 
        env.sentiment_reliabilities, env.gc_bonus, env.dd_penalty
    )
    
    state, _ = val_env.reset()
    
    for step in range(min(400, len(val_data)-50)):
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            action_probs, _ = agent.policy_old(state_tensor)
            action = torch.argmax(action_probs).item()
        
        state, _, done, _, _ = val_env.step(action)
        if done:
            break
    
    # Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ ÐºÐ¾Ð¼Ð±Ð¸Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½ÑƒÑŽ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÑƒ
    stats = val_env.get_portfolio_stats()
    return_score = stats['total_return']
    sharpe_score = stats.get('sharpe_ratio', 0) * 0.1  # Ð‘Ð¾Ð½ÑƒÑ Ð·Ð° Sharpe
    win_rate_score = stats.get('win_rate', 0) * 0.1    # Ð‘Ð¾Ð½ÑƒÑ Ð·Ð° Win Rate
    
    combined_score = return_score + sharpe_score + win_rate_score
    return combined_score

def run_comprehensive_optimization():
    """Ð—Ð°Ð¿ÑƒÑÐº Ð¿Ð¾Ð»Ð½Ð¾Ð¹ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸"""
    print("ðŸ”¬ ÐšÐžÐœÐŸÐ›Ð•ÐšÐ¡ÐÐÐ¯ OPTUNA ÐžÐŸÐ¢Ð˜ÐœÐ˜Ð—ÐÐ¦Ð˜Ð¯")
    print("=" * 50)
    print("ðŸŽ¯ ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼:")
    print("  â€¢ RL Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ (lr, gamma, hidden_dim, lookback)")
    print("  â€¢ Sentiment Ð²ÐµÑÐ° (5 Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð²)")
    print("  â€¢ Sentiment reliability (5 Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹)")
    print("  â€¢ Reward Ð¼Ð¾Ð´Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ñ‹")
    print("=" * 50)
    
    study = optuna.create_study(direction='maximize')
    
    # Ð Ð°Ð·Ð½Ñ‹Ðµ ÑÑ‚Ð°Ð´Ð¸Ð¸ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸
    print("ðŸš€ Ð¡Ñ‚Ð°Ð´Ð¸Ñ 1: Ð‘Ñ‹ÑÑ‚Ñ€Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº (50 trials)")
    study.optimize(objective, n_trials=50)
    
    print("ðŸŽ¯ Ð¡Ñ‚Ð°Ð´Ð¸Ñ 2: Ð¡Ñ€ÐµÐ´Ð½Ð¸Ð¹ Ð¿Ð¾Ð¸ÑÐº (100 trials)")
    study.optimize(objective, n_trials=100)
    
    print("ðŸ† Ð¡Ñ‚Ð°Ð´Ð¸Ñ 3: Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° (50 trials)")
    study.optimize(objective, n_trials=50)
    
    # Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹
    print("\nðŸ† Ð›Ð£Ð§Ð¨Ð˜Ð• ÐŸÐÐ ÐÐœÐ•Ð¢Ð Ð«:")
    print("=" * 50)
    best_params = study.best_trial.params
    
    print(f"ðŸ“ˆ Best Score: {study.best_trial.value:.4f}")
    print("\nðŸ¤– RL ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹:")
    for param in ['learning_rate', 'gamma', 'hidden_dim', 'lookback_window', 'episodes']:
        if param in best_params:
            print(f"  {param}: {best_params[param]}")
    
    print("\nðŸ§  Sentiment Ð’ÐµÑÐ°:")
    for param in ['marketaux_weight', 'fear_greed_weight', 'golden_cross_weight', 'vix_weight']:
        if param in best_params:
            print(f"  {param}: {best_params[param]:.3f}")
    
    print("\nðŸ” Sentiment Reliability:")
    for param in ['marketaux_reliability', 'fear_greed_reliability', 'golden_cross_reliability']:
        if param in best_params:
            print(f"  {param}: {best_params[param]:.3f}")
    
    print("\nðŸŽ Reward ÐœÐ¾Ð´Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ñ‹:")
    for param in ['gc_reward_bonus', 'drawdown_penalty']:
        if param in best_params:
            print(f"  {param}: {best_params[param]:.1f}")
    
    return study.best_trial.params

if __name__ == "__main__":
    best_params = run_comprehensive_optimization()
    
    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð»ÑƒÑ‡ÑˆÐ¸Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹
    import json
    with open('best_optuna_params.json', 'w') as f:
        json.dump(best_params, f, indent=2)
    
    print(f"\nðŸ’¾ Ð›ÑƒÑ‡ÑˆÐ¸Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð² best_optuna_params.json")