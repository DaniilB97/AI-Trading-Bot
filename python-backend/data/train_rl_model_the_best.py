#!/usr/bin/env python3
"""
–û–ë–ù–û–í–õ–ï–ù–ù–´–ô —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è RL –∞–≥–µ–Ω—Ç–∞ –ø–æ–¥ –Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Golden Cross, –≥–æ—Ç–æ–≤—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∏ multi-asset –¥–∞–Ω–Ω—ã–µ
"""
import gymnasium as gym
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import logging
from datetime import datetime
from sklearn.preprocessing import StandardScaler
import pickle

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- –ë–ª–æ–∫ 1: –û–ë–ù–û–í–õ–ï–ù–ù–ê–Ø –°—Ä–µ–¥–∞ –¥–ª—è –Ω–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö ---
class EnhancedGoldTradingEnvironment(gym.Env):
    """
    –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Å—Ä–µ–¥–∞ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –Ω–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö:
    1. GOLD_Close –≤–º–µ—Å—Ç–æ Close
    2. –ì–æ—Ç–æ–≤—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (–Ω–µ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∑–∞–Ω–æ–≤–æ)
    3. Golden Cross —Å–∏–≥–Ω–∞–ª—ã
    4. Multi-asset –¥–∞–Ω–Ω—ã–µ
    5. –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π sentiment –∞–Ω–∞–ª–∏–∑
    """
    def __init__(self, data, initial_balance=10000, commission=0.001, lookback_window=24):
        super().__init__()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –≤ –Ω–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        required_columns = ['GOLD_Close', 'sentiment']
        missing_columns = [col for col in required_columns if col not in data.columns]
        if missing_columns:
            raise ValueError(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
            
        self.data = data.copy()
        self.initial_balance = initial_balance
        self.commission = commission
        self.lookback_window = lookback_window
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å window_size –¥–ª—è –Ω–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
        self.window_size = lookback_window  # –ê–ª–∏–∞—Å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∏—á–∏ –∏–∑ –ì–û–¢–û–í–û–ì–û –¥–∞—Ç–∞—Å–µ—Ç–∞ (–Ω–µ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∑–∞–Ω–æ–≤–æ!)
        self.feature_columns = self._define_feature_columns()
        logger.info(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ñ–∏—á–∏ ({len(self.feature_columns)}): {self.feature_columns}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Ñ–∏—á–µ–π
        self._validate_features()
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        self._prepare_data()
        
        # –°–û–ó–î–ê–ï–ú –ú–ê–¢–ï–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† SENTIMENT –û–î–ò–ù –†–ê–ó!
        try:
            from mathematical_sentiment_model import MathematicalSentimentAnalyzer
            self.sentiment_analyzer = MathematicalSentimentAnalyzer()
            logger.info("üßÆ –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä sentiment —Å–æ–∑–¥–∞–Ω")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä sentiment: {e}")
            self.sentiment_analyzer = None
        
        # Gym spaces
        self.action_space = gym.spaces.Discrete(3)  # 0: Hold, 1: Buy, 2: Sell
        
        # –†–∞–∑–º–µ—Ä observation space
        n_features = len(self.feature_columns)
        n_portfolio_state = 4  # –±–∞–ª–∞–Ω—Å, –ø–æ–∑–∏—Ü–∏—è, —Ä–∞–∑–º–µ—Ä, –Ω–µ—Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π PnL
        observation_space_size = n_features * lookback_window + n_portfolio_state
        
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf, shape=(observation_space_size,), dtype=np.float32
        )
        
        logger.info(f"üéØ Environment –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        logger.info(f"   üìä –î–∞–Ω–Ω—ã—Ö: {len(self.data)} –∑–∞–ø–∏—Å–µ–π")
        logger.info(f"   üß† Observation space: {observation_space_size}")
        logger.info(f"   ‚ö° Action space: {self.action_space.n}")

    def _define_feature_columns(self):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∏—á–∏ –∏–∑ –≥–æ—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        # –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
        base_features = [
            'GOLD_Close',           # –û—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–Ω–∞
            'DXY_Close',            # –ò–Ω–¥–µ–∫—Å –¥–æ–ª–ª–∞—Ä–∞
            'VIX_Close',            # –ò–Ω–¥–µ–∫—Å —Å—Ç—Ä–∞—Ö–∞
            'sentiment'             # –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏—è —Ä—ã–Ω–∫–∞
        ]
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (—É–∂–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–µ)
        technical_features = [
            'STOCHk_14_3_3',        # Stochastic %K
            'CCI_14_0.015',         # Commodity Channel Index
            'Price_Change_1',       # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã –∑–∞ 1 —á–∞—Å
            'Price_Change_5',       # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã –∑–∞ 5 —á–∞—Å–æ–≤
            'Volatility_24',        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            'DXY_change',           # –ò–∑–º–µ–Ω–µ–Ω–∏–µ –¥–æ–ª–ª–∞—Ä–∞
            'VIX_change',           # –ò–∑–º–µ–Ω–µ–Ω–∏–µ VIX
        ]
        
        # Golden Cross —Ñ–∏—á–∏ (–ù–û–í–´–ï!)
        golden_cross_features = [
            'golden_cross_trend',    # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞ (-1, 0, 1)
            'ma_spread_normalized',  # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É MA
        ]
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∏—á–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        optional_features = [
            'RSI_14',               # RSI (–µ—Å–ª–∏ –µ—Å—Ç—å)
            'ATR_14',               # Average True Range (–µ—Å–ª–∏ –µ—Å—Ç—å)
        ]
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∏—á–∏
        all_features = base_features + technical_features + golden_cross_features + optional_features
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –µ—Å—Ç—å –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
        available_features = [col for col in all_features if col in self.data.columns]
        
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ñ–∏—á–µ–π: {len(available_features)} –∏–∑ {len(all_features)} –≤–æ–∑–º–æ–∂–Ω—ã—Ö")
        return available_features

    def _validate_features(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –≤—Å–µ—Ö —Ñ–∏—á–µ–π"""
        missing_features = [col for col in self.feature_columns if col not in self.data.columns]
        if missing_features:
            logger.warning(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ñ–∏—á–∏: {missing_features}")
            # –£–±–∏—Ä–∞–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ñ–∏—á–∏
            self.feature_columns = [col for col in self.feature_columns if col in self.data.columns]
        
        logger.info(f"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞. –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏: {len(self.feature_columns)}")

    def _prepare_data(self):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –ø–µ—Ä–µ—Å—á–µ—Ç–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        logger.info("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        # –£–±–∏—Ä–∞–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
        initial_length = len(self.data)
        self.data = self.data.dropna(subset=self.feature_columns)
        final_length = len(self.data)
        
        if final_length < initial_length:
            logger.warning(f"‚ö†Ô∏è –£–¥–∞–ª–µ–Ω–æ {initial_length - final_length} —Å—Ç—Ä–æ–∫ —Å NaN")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏—á–µ–π (–∫—Ä–æ–º–µ —Ü–µ–Ω –∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö)
        self._normalize_features()
        
        logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã: {len(self.data)} –∑–∞–ø–∏—Å–µ–π –≥–æ—Ç–æ–≤–æ –∫ –æ–±—É—á–µ–Ω–∏—é")

    def _normalize_features(self):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏—á–µ–π –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
        logger.info("üìä –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏—á–µ–π...")
        
        # –§–∏—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º (—Ü–µ–Ω—ã –∏ Golden Cross trend)
        skip_normalization = ['GOLD_Close', 'DXY_Close', 'VIX_Close', 'golden_cross_trend']
        
        for col in self.feature_columns:
            if col not in skip_normalization and col in self.data.columns:
                # Z-score –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                mean_val = self.data[col].mean()
                std_val = self.data[col].std()
                
                if std_val > 1e-9:  # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
                    self.data[col] = (self.data[col] - mean_val) / std_val
                    logger.debug(f"   ‚úì –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω {col}: mean={mean_val:.3f}, std={std_val:.3f}")
                else:
                    logger.warning(f"   ‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è {col}: std —Å–ª–∏—à–∫–æ–º –º–∞–ª")
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Ü–µ–Ω (–ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ)
        for price_col in ['GOLD_Close', 'DXY_Close', 'VIX_Close']:
            if price_col in self.data.columns:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
                first_value = self.data[price_col].iloc[0]
                self.data[f'{price_col}_normalized'] = self.data[price_col] / first_value
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ñ–∏—á–∏ (–∑–∞–º–µ–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—É—é —Ü–µ–Ω—É)
                if price_col in self.feature_columns:
                    idx = self.feature_columns.index(price_col)
                    self.feature_columns[idx] = f'{price_col}_normalized'
        
        logger.info("‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

    def reset(self, seed=None):
        """–°–±—Ä–æ—Å —Å—Ä–µ–¥—ã –≤ –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ - –ö–ê–ö –í –û–†–ò–ì–ò–ù–ê–õ–ï"""
        super().reset(seed=seed)
        self.current_step = self.lookback_window
        self.balance = self.initial_balance
        self.position = 0
        self.position_size = 0.0
        self.entry_price = 0.0
        self.trade_count = 0
        
        return self._get_observation(), {}

    def _get_observation(self):
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â–µ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ (—Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ä—ã–Ω–∫–∞ + —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Ä—Ç—Ñ–µ–ª—è).
        –†–æ–±–∞—Å—Ç–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—à–∏–±–æ–∫ –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º sentiment.
        """
        # 1. –í–°–ï–ì–î–ê –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∞–∑–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ä—ã–Ω–∫–∞ –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å
        start = max(0, self.current_step - self.lookback_window + 1)
        end = self.current_step + 1
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ä–µ–∑ –¥–∞–Ω–Ω—ã—Ö
        market_data_slice = self.data.iloc[start:end][self.feature_columns]
        market_state = market_data_slice.values.flatten()
        
        # –ï—Å–ª–∏ –≤ –Ω–∞—á–∞–ª–µ —ç–ø–∏–∑–æ–¥–∞ –¥–∞–Ω–Ω—ã—Ö –º–µ–Ω—å—à–µ, —á–µ–º —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞, –¥–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
        expected_size = self.lookback_window * len(self.feature_columns)
        if len(market_state) < expected_size:
            padding = np.zeros(expected_size - len(market_state))
            market_state = np.concatenate([padding, market_state])
            logger.debug(f"üîß –î–æ–±–∞–≤–ª–µ–Ω padding: {len(padding)} –Ω—É–ª–µ–π")
        
        # 2. –¢–ï–ü–ï–†–¨ –ø—ã—Ç–∞–µ–º—Å—è —É–ª—É—á—à–∏—Ç—å state —Å –ø–æ–º–æ—â—å—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –£–ñ–ï –°–û–ó–î–ê–ù–ù–´–ô –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä (–Ω–µ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π!)
            if self.sentiment_analyzer is not None:
                from mathematical_sentiment_model import get_enhanced_sentiment
                
                current_data_row = self.data.iloc[self.current_step]
                enhanced_sentiment, confidence = get_enhanced_sentiment(self.sentiment_analyzer, current_data_row)
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π sentiment, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —É–≤–µ—Ä–µ–Ω–∞
                CONFIDENCE_THRESHOLD = 0.5 
                if confidence > CONFIDENCE_THRESHOLD and 'sentiment' in self.feature_columns:
                    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é, —á—Ç–æ–±—ã –Ω–µ –∏–∑–º–µ–Ω—è—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    enhanced_market_state = market_state.reshape(self.lookback_window, len(self.feature_columns)).copy()
                    
                    # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å –∫–æ–ª–æ–Ω–∫–∏ 'sentiment', —á—Ç–æ–±—ã –∑–∞–º–µ–Ω–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ
                    sentiment_column_index = self.feature_columns.index('sentiment')
                    
                    # –ó–∞–º–µ–Ω—è–µ–º sentiment –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–º —à–∞–≥–µ –æ–∫–Ω–∞
                    enhanced_market_state[-1, sentiment_column_index] = enhanced_sentiment
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º market_state, —Å–≥–ª–∞–∂–∏–≤–∞—è –µ–≥–æ –æ–±—Ä–∞—Ç–Ω–æ –≤ 1D –º–∞—Å—Å–∏–≤
                    market_state = enhanced_market_state.flatten()
                    logger.debug(f"üß† Sentiment —É–ª—É—á—à–µ–Ω –¥–æ {enhanced_sentiment:.3f} —Å confidence {confidence:.2f}")
                else:
                    logger.debug(f"‚ö†Ô∏è Sentiment –Ω–µ —É–ª—É—á—à–µ–Ω: confidence {confidence:.2f} < {CONFIDENCE_THRESHOLD}")
            else:
                logger.debug("‚ö†Ô∏è –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä sentiment –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
                
        except Exception as e:
            # –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π market_state
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π sentiment: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π.")
        
        # 3. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Ä—Ç—Ñ–µ–ª—è (–ü–†–ê–í–ò–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø)
        current_price = self.data['GOLD_Close'].iloc[self.current_step]
        unrealized_pnl = 0
        if self.position == 1 and self.entry_price > 0:
            unrealized_pnl = (current_price - self.entry_price) / self.entry_price
        
        portfolio_state = np.array([
            self.balance / self.initial_balance,  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –±–∞–ª–∞–Ω—Å
            float(self.position),                 # –¢–µ–∫—É—â–∞—è –ø–æ–∑–∏—Ü–∏—è (0 –∏–ª–∏ 1)
            self.position_size / max(1.0, self.balance / current_price) if current_price > 0 else 0,  # –†–∞–∑–º–µ—Ä –ø–æ–∑–∏—Ü–∏–∏
            unrealized_pnl                       # –ù–µ—Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π P&L
        ], dtype=np.float32)
        
        # 4. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è. –≠—Ç–∞ —Å—Ç—Ä–æ–∫–∞ —Ç–µ–ø–µ—Ä—å –≤—Å–µ–≥–¥–∞ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å.
        observation = np.concatenate([market_state, portfolio_state]).astype(np.float32)
        
        # 5. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN/inf –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º
        if np.any(np.isnan(observation)) or np.any(np.isinf(observation)):
            logger.warning("‚ö†Ô∏è NaN/inf –≤ observation, –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
            observation = np.nan_to_num(observation, nan=0.0, posinf=1.0, neginf=-1.0)
        
        # 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        expected_obs_size = self.observation_space.shape[0]
        if observation.shape[0] != expected_obs_size:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ observation! –û–∂–∏–¥–∞–ª–æ—Å—å {expected_obs_size}, –ø–æ–ª—É—á–µ–Ω–æ {observation.shape[0]}")
            # –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø—Ä–∞–≤–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
            if observation.shape[0] < expected_obs_size:
                # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
                padding = np.zeros(expected_obs_size - observation.shape[0])
                observation = np.concatenate([observation, padding])
            else:
                # –û–±—Ä–µ–∑–∞–µ–º
                observation = observation[:expected_obs_size]
            logger.warning(f"üîß –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –¥–æ {observation.shape[0]}")
            
        return observation.astype(np.float32)

    def step(self, action):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç —à–∞–≥ –≤ —Å—Ä–µ–¥–µ - –û–†–ò–ì–ò–ù–ê–õ–¨–ù–ê–Ø –õ–û–ì–ò–ö–ê + –õ–û–ì–ò–ß–ù–´–ï –£–õ–£–ß–®–ï–ù–ò–Ø"""
        current_price = self.data['GOLD_Close'].iloc[self.current_step]
        reward = 0
        
        if action == 1: # Buy
            if self.position <= 0: 
                self.position = 1
                self.position_size = (self.balance * 0.95) / current_price if current_price > 0 else 0
                self.entry_price = current_price
                self.balance -= self.position_size * current_price * self.commission
                self.trade_count += 1
                reward -= 1  # –û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ô —à—Ç—Ä–∞—Ñ –∑–∞ –æ—Ç–∫—Ä—ã—Ç–∏–µ –ø–æ–∑–∏—Ü–∏–∏
                
        elif action == 2: # Sell (–≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ, —Ç–æ–ª—å–∫–æ –∑–∞–∫—Ä—ã—Ç–∏–µ –ª–æ–Ω–≥–∞)
            if self.position == 1:
                profit = (current_price - self.entry_price) * self.position_size
                self.balance += profit - (current_price * self.position_size * self.commission)
                reward += profit / self.initial_balance * 100  # –û–†–ò–ì–ò–ù–ê–õ–¨–ù–ê–Ø –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ –ø—Ä–∏–±—ã–ª—å
                self.position = 0
                self.position_size = 0

        # –û–†–ò–ì–ò–ù–ê–õ–¨–ù–ê–Ø –õ–û–ì–ò–ö–ê: –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ unrealized profit –ø–æ–∫–∞ –ø–æ–∑–∏—Ü–∏—è –æ—Ç–∫—Ä—ã—Ç–∞
        if self.position != 0:
            if (self.current_step + 1) < len(self.data):
                next_price = self.data['GOLD_Close'].iloc[self.current_step + 1]
                price_diff = (next_price - current_price) * self.position
                reward += price_diff / current_price * 100  # –≠–¢–û –£–ñ–ï –ë–´–õ–û –í –û–†–ò–ì–ò–ù–ê–õ–ï!

        # ‚ú® –õ–û–ì–ò–ß–ù–û–ï –î–û–ë–ê–í–õ–ï–ù–ò–ï 1: Golden Cross —Å–º–∞—Ä—Ç-–Ω–∞–≥—Ä–∞–¥—ã
        if 'golden_cross_trend' in self.data.columns:
            gc_trend = self.data['golden_cross_trend'].iloc[self.current_step]
            
            # –£–º–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –∑–∞ —Ç–æ—Ä–≥–æ–≤–ª—é –ø–æ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–º—É —Ç—Ä–µ–Ω–¥—É
            if action == 1 and gc_trend > 0:  # Buy –≤ –±—ã—á—å–µ–º —Ç—Ä–µ–Ω–¥–µ
                reward += 2  # –ù–µ–±–æ–ª—å—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞
            elif action == 2 and gc_trend < 0:  # Sell –≤ –º–µ–¥–≤–µ–∂—å–µ–º —Ç—Ä–µ–Ω–¥–µ  
                reward += 2  # –ù–µ–±–æ–ª—å—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞
            elif action == 1 and gc_trend < 0:  # Buy –ø—Ä–æ—Ç–∏–≤ —Ç—Ä–µ–Ω–¥–∞
                reward -= 3  # –®—Ç—Ä–∞—Ñ –∑–∞ —Ç–æ—Ä–≥–æ–≤–ª—é –ø—Ä–æ—Ç–∏–≤ —Ç—Ä–µ–Ω–¥–∞

        # ‚ú® –õ–û–ì–ò–ß–ù–û–ï –î–û–ë–ê–í–õ–ï–ù–ò–ï 2: –®—Ç—Ä–∞—Ñ—ã –∑–∞ –æ–ø–∞—Å–Ω—ã–µ –ø—Ä–æ—Å–∞–¥–∫–∏
        current_equity = self.balance
        if self.position == 1:
            current_equity += self.position_size * current_price
        
        equity_ratio = current_equity / self.initial_balance
        if equity_ratio < 0.7:  # –ü—Ä–æ—Å–∞–¥–∫–∞ –±–æ–ª—å—à–µ 30%
            reward -= 10  # –®—Ç—Ä–∞—Ñ –∑–∞ —Ä–∏—Å–∫
        elif equity_ratio < 0.5:  # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞
            reward -= 25  # –ë–æ–ª—å—à–æ–π —à—Ç—Ä–∞—Ñ

        self.current_step += 1
        done = self.current_step >= len(self.data) - 2 or self.balance < self.initial_balance * 0.5  # –û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ï —É—Å–ª–æ–≤–∏—è
        
        return self._get_observation(), reward, done, False, {}

    def get_portfolio_stats(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ—Ä—Ç—Ñ–µ–ª—è - –£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø"""
        current_price = self.data['GOLD_Close'].iloc[self.current_step - 1]
        current_equity = self.balance
        
        if self.position == 1:
            current_equity += self.position_size * current_price
        
        total_return = (current_equity - self.initial_balance) / self.initial_balance
        
        return {
            'balance': self.balance,
            'position': self.position,
            'current_equity': current_equity,
            'total_return': total_return,
            'trade_count': self.trade_count
        }

# --- –ë–ª–æ–∫ 2: PPO –ê–≥–µ–Ω—Ç (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô) ---
class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super(ActorCritic, self).__init__()
        self.actor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim), 
            nn.ReLU(), 
            nn.Linear(hidden_dim, hidden_dim), 
            nn.ReLU(), 
            nn.Linear(hidden_dim, action_dim), 
            nn.Softmax(dim=-1)
        )
        self.critic = nn.Sequential(
            nn.Linear(state_dim, hidden_dim), 
            nn.ReLU(), 
            nn.Linear(hidden_dim, hidden_dim), 
            nn.ReLU(), 
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, state):
        return self.actor(state), self.critic(state)

class Memory:
    def __init__(self): 
        self.actions, self.states, self.logprobs, self.rewards, self.is_terminals = [], [], [], [], []
    
    def clear(self): 
        del self.actions[:]; del self.states[:]; del self.logprobs[:]; del self.rewards[:]; del self.is_terminals[:]

class PPOAgent:
    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, K_epochs=4, eps_clip=0.2):
        self.gamma, self.eps_clip, self.K_epochs = gamma, eps_clip, K_epochs
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        self.policy = ActorCritic(state_dim, action_dim).to(self.device)
        self.policy_old = ActorCritic(state_dim, action_dim).to(self.device)
        
        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)
        self.policy_old.load_state_dict(self.policy.state_dict())
        self.MseLoss = nn.MSELoss()
        
        logger.info(f"ü§ñ PPO Agent –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –Ω–∞ {self.device}")
        
    def select_action(self, state, memory):
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).to(self.device).unsqueeze(0)
            action_probs, _ = self.policy_old(state_tensor)
            dist = Categorical(action_probs)
            action = dist.sample()
            
            memory.states.append(state_tensor)
            memory.actions.append(action)
            memory.logprobs.append(dist.log_prob(action))
            
        return action.item()

    def update(self, memory):
        # –í—ã—á–∏—Å–ª—è–µ–º discounted rewards
        rewards = []
        discounted_reward = 0
        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):
            if is_terminal: 
                discounted_reward = 0
            discounted_reward = reward + (self.gamma * discounted_reward)
            rewards.insert(0, discounted_reward)
            
        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)

        old_states = torch.squeeze(torch.stack(memory.states, dim=0)).to(self.device).detach()
        old_actions = torch.squeeze(torch.stack(memory.actions, dim=0)).to(self.device).detach()
        old_logprobs = torch.squeeze(torch.stack(memory.logprobs, dim=0)).to(self.device).detach()

        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏
        for _ in range(self.K_epochs):
            action_probs, state_values = self.policy(old_states)
            state_values = torch.squeeze(state_values)

            dist = Categorical(action_probs)
            logprobs = dist.log_prob(old_actions)
            dist_entropy = dist.entropy()
            
            ratios = torch.exp(logprobs - old_logprobs.detach())
            advantages = rewards - state_values.detach()
            
            surr1 = ratios * advantages
            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages
            
            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy
            
            self.optimizer.zero_grad()
            loss.mean().backward()
            self.optimizer.step()
            
        self.policy_old.load_state_dict(self.policy.state_dict())
        return loss.mean().item()

# --- –ë–ª–æ–∫ 3: –ì–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è ---
def main():
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è RL Gold Trader —Å –Ω–æ–≤—ã–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    try:
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤
        possible_files = [
            'master_training_dataset.csv',
            'gold_data_with_sentiment_hourly.csv', 
            'automated_gold_data_with_sentiment.csv'
        ]
        
        data_df = None
        for filename in possible_files:
            try:
                data_df = pd.read_csv(filename, index_col=0, parse_dates=True)
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç: {filename}")
                break
            except FileNotFoundError:
                continue
        
        if data_df is None:
            raise FileNotFoundError("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω –Ω–∏ –æ–¥–∏–Ω –∏–∑ –æ–∂–∏–¥–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç–∞")
            
        logger.info(f"üìä –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {data_df.shape}")
        logger.info(f"üìã –ö–æ–ª–æ–Ω–∫–∏: {list(data_df.columns)}")
        logger.info(f"üìÖ –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: {data_df.index.min()} ‚Üí {data_df.index.max()}")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return

    # –°–æ–∑–¥–∞–µ–º —Å—Ä–µ–¥—É
    try:
        env = EnhancedGoldTradingEnvironment(data_df, lookback_window=24)
        state_dim = env.observation_space.shape[0]
        action_dim = env.action_space.n
        
        logger.info(f"üéØ –°—Ä–µ–¥–∞ —Å–æ–∑–¥–∞–Ω–∞:")
        logger.info(f"   State dimension: {state_dim}")
        logger.info(f"   Action dimension: {action_dim}")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ä–µ–¥—ã: {e}")
        return

    # –°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç–∞
    agent = PPOAgent(state_dim, action_dim, lr=1e-4, gamma=0.99)  # –£–º–µ–Ω—å—à–∏–ª–∏ learning rate
    memory = Memory()

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    num_episodes = 200  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è –Ω–∞—á–∞–ª–∞
    update_timestep = 1000  # –û–±–Ω–æ–≤–ª—è–µ–º —á–∞—â–µ
    max_timesteps = 10000
    
    logger.info(f"üéì –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ:")
    logger.info(f"   Episodes: {num_episodes}")
    logger.info(f"   Update frequency: {update_timestep}")
    logger.info(f"   Max timesteps per episode: {max_timesteps}")
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
    timestep = 0
    best_reward = -float('inf')
    
    for episode in range(1, num_episodes + 1):
        state, _ = env.reset()
        episode_reward = 0
        episode_steps = 0
        
        try:
            while episode_steps < max_timesteps:
                timestep += 1
                episode_steps += 1
                
                # –í—ã–±–∏—Ä–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
                action = agent.select_action(state, memory)
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º —à–∞–≥
                state, reward, done, _, _ = env.step(action)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å
                memory.rewards.append(reward)
                memory.is_terminals.append(done)
                episode_reward += reward
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–ª–∏—Ç–∏–∫—É
                if timestep % update_timestep == 0:
                    loss = agent.update(memory)
                    memory.clear()
                    logger.info(f"üîÑ Policy updated at timestep {timestep}. Loss: {loss:.4f}")
                
                if done:
                    break
                    
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —ç–ø–∏–∑–æ–¥–µ {episode}: {e}")
            break
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–ø–∏–∑–æ–¥–∞
        stats = env.get_portfolio_stats()
        
        logger.info(f"Episode {episode:3d} | "
                   f"Reward: {episode_reward:8.2f} | "
                   f"Return: {stats['total_return']*100:6.2f}% | "
                   f"Trades: {stats['trade_count']:3d} | "
                   f"Steps: {episode_steps:4d}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if episode_reward > best_reward:
            best_reward = episode_reward
            best_model_path = f"best_rl_gold_model_{datetime.now().strftime('%Y%m%d_%H%M')}.pth"
            torch.save(agent.policy_old.state_dict(), best_model_path)
            logger.info(f"üíæ –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {best_model_path}")

    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    final_model_path = f"final_rl_gold_model_{datetime.now().strftime('%Y%m%d_%H%M')}.pth"
    torch.save(agent.policy_old.state_dict(), final_model_path)
    
    logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    logger.info(f"üíæ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {final_model_path}")
    logger.info(f"üèÜ –õ—É—á—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {best_reward:.2f}")

if __name__ == '__main__':
    main()