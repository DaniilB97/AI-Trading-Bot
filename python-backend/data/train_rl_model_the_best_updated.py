#!/usr/bin/env python3
"""
–ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–´–ô —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è RL –∞–≥–µ–Ω—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π reward —Ñ—É–Ω–∫—Ü–∏–µ–π
–í–∫–ª—é—á–∞–µ—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π sentiment –∞–Ω–∞–ª–∏–∑, Sharpe ratio –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã
"""
import gymnasium as gym
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import logging
from datetime import datetime
from sklearn.preprocessing import StandardScaler
import pickle

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- –ë–ª–æ–∫ 1: –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–ê–Ø –°—Ä–µ–¥–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π reward —Ñ—É–Ω–∫—Ü–∏–µ–π ---
class EnhancedGoldTradingEnvironment(gym.Env):
    """
    –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ä–µ–¥–∞ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:
    1. –ù–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö (GOLD_Close, –≥–æ—Ç–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã)
    2. –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ sentiment –∞–Ω–∞–ª–∏–∑–∞
    3. Sharpe ratio –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    4. –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π reward —Ñ—É–Ω–∫—Ü–∏–∏
    5. Golden Cross —Å–∏–≥–Ω–∞–ª–æ–≤
    """
    def __init__(self, data, initial_balance=10000, commission=0.001, lookback_window=24):
        super().__init__()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –≤ –Ω–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        required_columns = ['GOLD_Close', 'sentiment']
        missing_columns = [col for col in required_columns if col not in data.columns]
        if missing_columns:
            raise ValueError(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
            
        self.data = data.copy()
        self.initial_balance = initial_balance
        self.commission = commission
        self.lookback_window = lookback_window
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å window_size –¥–ª—è –Ω–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
        self.window_size = lookback_window  # –ê–ª–∏–∞—Å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        
        # üî• –î–û–ë–ê–í–õ–Ø–ï–ú –¥–ª—è Sharpe –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.returns_history = []
        self.trades_history = []
        self.portfolio_values = []
        self.risk_free_rate = 0.02 / (365 * 24)  # –ß–∞—Å–æ–≤–∞—è –±–µ–∑—Ä–∏—Å–∫–æ–≤–∞—è —Å—Ç–∞–≤–∫–∞
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∏—á–∏ –∏–∑ –ì–û–¢–û–í–û–ì–û –¥–∞—Ç–∞—Å–µ—Ç–∞ (–Ω–µ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∑–∞–Ω–æ–≤–æ!)
        self.feature_columns = self._define_feature_columns()
        logger.info(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ñ–∏—á–∏ ({len(self.feature_columns)}): {self.feature_columns}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Ñ–∏—á–µ–π
        self._validate_features()
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        self._prepare_data()
        
        # –°–û–ó–î–ê–ï–ú –ú–ê–¢–ï–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† SENTIMENT –û–î–ò–ù –†–ê–ó!
        try:
            from mathematical_sentiment_model import MathematicalSentimentAnalyzer
            self.sentiment_analyzer = MathematicalSentimentAnalyzer()
            logger.info("üßÆ –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä sentiment —Å–æ–∑–¥–∞–Ω")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä sentiment: {e}")
            self.sentiment_analyzer = None
        
        # Gym spaces
        self.action_space = gym.spaces.Discrete(3)  # 0: Hold, 1: Buy, 2: Sell
        
        # –†–∞–∑–º–µ—Ä observation space
        n_features = len(self.feature_columns)
        n_portfolio_state = 4  # –±–∞–ª–∞–Ω—Å, –ø–æ–∑–∏—Ü–∏—è, —Ä–∞–∑–º–µ—Ä, –Ω–µ—Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π PnL
        observation_space_size = n_features * lookback_window + n_portfolio_state
        
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf, shape=(observation_space_size,), dtype=np.float32
        )
        
        logger.info(f"üéØ Environment –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        logger.info(f"   üìä –î–∞–Ω–Ω—ã—Ö: {len(self.data)} –∑–∞–ø–∏—Å–µ–π")
        logger.info(f"   üß† Observation space: {observation_space_size}")
        logger.info(f"   ‚ö° Action space: {self.action_space.n}")

    def _define_feature_columns(self):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∏—á–∏ –∏–∑ –≥–æ—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        # –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
        base_features = [
            'GOLD_Close',           # –û—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–Ω–∞
            'DXY_Close',            # –ò–Ω–¥–µ–∫—Å –¥–æ–ª–ª–∞—Ä–∞
            'VIX_Close',            # –ò–Ω–¥–µ–∫—Å —Å—Ç—Ä–∞—Ö–∞
            'sentiment'             # –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏—è —Ä—ã–Ω–∫–∞
        ]
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (—É–∂–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–µ)
        technical_features = [
            'STOCHk_14_3_3',        # Stochastic %K
            'CCI_14_0.015',         # Commodity Channel Index
            'Price_Change_1',       # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã –∑–∞ 1 —á–∞—Å
            'Price_Change_5',       # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã –∑–∞ 5 —á–∞—Å–æ–≤
            'Volatility_24',        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            'DXY_change',           # –ò–∑–º–µ–Ω–µ–Ω–∏–µ –¥–æ–ª–ª–∞—Ä–∞
            'VIX_change',           # –ò–∑–º–µ–Ω–µ–Ω–∏–µ VIX
        ]
        
        # Golden Cross —Ñ–∏—á–∏ (–ù–û–í–´–ï!)
        golden_cross_features = [
            'golden_cross_trend',    # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞ (-1, 0, 1)
            'ma_spread_normalized',  # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É MA
        ]
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∏—á–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        optional_features = [
            'RSI_14',               # RSI (–µ—Å–ª–∏ –µ—Å—Ç—å)
            'ATR_14',               # Average True Range (–µ—Å–ª–∏ –µ—Å—Ç—å)
        ]
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∏—á–∏
        all_features = base_features + technical_features + golden_cross_features + optional_features
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –µ—Å—Ç—å –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
        available_features = [col for col in all_features if col in self.data.columns]
        
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ñ–∏—á–µ–π: {len(available_features)} –∏–∑ {len(all_features)} –≤–æ–∑–º–æ–∂–Ω—ã—Ö")
        return available_features

    def _validate_features(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –≤—Å–µ—Ö —Ñ–∏—á–µ–π"""
        missing_features = [col for col in self.feature_columns if col not in self.data.columns]
        if missing_features:
            logger.warning(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ñ–∏—á–∏: {missing_features}")
            # –£–±–∏—Ä–∞–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ñ–∏—á–∏
            self.feature_columns = [col for col in self.feature_columns if col in self.data.columns]
        
        logger.info(f"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞. –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏: {len(self.feature_columns)}")

    def _prepare_data(self):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –ø–µ—Ä–µ—Å—á–µ—Ç–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        logger.info("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        # –£–±–∏—Ä–∞–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
        initial_length = len(self.data)
        self.data = self.data.dropna(subset=self.feature_columns)
        final_length = len(self.data)
        
        if final_length < initial_length:
            logger.warning(f"‚ö†Ô∏è –£–¥–∞–ª–µ–Ω–æ {initial_length - final_length} —Å—Ç—Ä–æ–∫ —Å NaN")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏—á–µ–π (–∫—Ä–æ–º–µ —Ü–µ–Ω –∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö)
        self._normalize_features()
        
        logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã: {len(self.data)} –∑–∞–ø–∏—Å–µ–π –≥–æ—Ç–æ–≤–æ –∫ –æ–±—É—á–µ–Ω–∏—é")

    def _normalize_features(self):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏—á–µ–π –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
        logger.info("üìä –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏—á–µ–π...")
        
        # –§–∏—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º (—Ü–µ–Ω—ã –∏ Golden Cross trend)
        skip_normalization = ['GOLD_Close', 'DXY_Close', 'VIX_Close', 'golden_cross_trend']
        
        for col in self.feature_columns:
            if col not in skip_normalization and col in self.data.columns:
                # Z-score –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                mean_val = self.data[col].mean()
                std_val = self.data[col].std()
                
                if std_val > 1e-9:  # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
                    self.data[col] = (self.data[col] - mean_val) / std_val
                    logger.debug(f"   ‚úì –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω {col}: mean={mean_val:.3f}, std={std_val:.3f}")
                else:
                    logger.warning(f"   ‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è {col}: std —Å–ª–∏—à–∫–æ–º –º–∞–ª")
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Ü–µ–Ω (–ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ)
        for price_col in ['GOLD_Close', 'DXY_Close', 'VIX_Close']:
            if price_col in self.data.columns:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
                first_value = self.data[price_col].iloc[0]
                self.data[f'{price_col}_normalized'] = self.data[price_col] / first_value
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ñ–∏—á–∏ (–∑–∞–º–µ–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—É—é —Ü–µ–Ω—É)
                if price_col in self.feature_columns:
                    idx = self.feature_columns.index(price_col)
                    self.feature_columns[idx] = f'{price_col}_normalized'
        
        logger.info("‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

    def reset(self, seed=None):
        """–°–±—Ä–æ—Å —Å—Ä–µ–¥—ã –≤ –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ"""
        super().reset(seed=seed)
        self.current_step = self.lookback_window
        self.balance = self.initial_balance
        self.position = 0
        self.position_size = 0.0
        self.entry_price = 0.0
        self.trade_count = 0
        
        # –°–±—Ä–æ—Å –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è Sharpe ratio
        self.returns_history = []
        self.trades_history = []
        self.portfolio_values = []
        
        return self._get_observation(), {}

    def _get_observation(self):
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â–µ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ —Å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º sentiment –∞–Ω–∞–ª–∏–∑–æ–º
        """
        # 1. –í–°–ï–ì–î–ê –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∞–∑–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ä—ã–Ω–∫–∞ –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å
        start = max(0, self.current_step - self.lookback_window + 1)
        end = self.current_step + 1
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ä–µ–∑ –¥–∞–Ω–Ω—ã—Ö
        market_data_slice = self.data.iloc[start:end][self.feature_columns]
        market_state = market_data_slice.values.flatten()
        
        # –ï—Å–ª–∏ –≤ –Ω–∞—á–∞–ª–µ —ç–ø–∏–∑–æ–¥–∞ –¥–∞–Ω–Ω—ã—Ö –º–µ–Ω—å—à–µ, —á–µ–º —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞, –¥–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
        expected_size = self.lookback_window * len(self.feature_columns)
        if len(market_state) < expected_size:
            padding = np.zeros(expected_size - len(market_state))
            market_state = np.concatenate([padding, market_state])
            logger.debug(f"üîß –î–æ–±–∞–≤–ª–µ–Ω padding: {len(padding)} –Ω—É–ª–µ–π")
        
        # 2. –¢–ï–ü–ï–†–¨ –ø—ã—Ç–∞–µ–º—Å—è —É–ª—É—á—à–∏—Ç—å state —Å –ø–æ–º–æ—â—å—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –£–ñ–ï –°–û–ó–î–ê–ù–ù–´–ô –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä (–Ω–µ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π!)
            if self.sentiment_analyzer is not None:
                from mathematical_sentiment_model import get_enhanced_sentiment
                
                current_data_row = self.data.iloc[self.current_step]
                enhanced_sentiment, confidence = get_enhanced_sentiment(self.sentiment_analyzer, current_data_row)
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π sentiment, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —É–≤–µ—Ä–µ–Ω–∞
                CONFIDENCE_THRESHOLD = 0.6
                if confidence > CONFIDENCE_THRESHOLD and 'sentiment' in self.feature_columns:
                    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é, —á—Ç–æ–±—ã –Ω–µ –∏–∑–º–µ–Ω—è—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    enhanced_market_state = market_state.reshape(self.lookback_window, len(self.feature_columns)).copy()
                    
                    # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å –∫–æ–ª–æ–Ω–∫–∏ 'sentiment', —á—Ç–æ–±—ã –∑–∞–º–µ–Ω–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ
                    sentiment_column_index = self.feature_columns.index('sentiment')
                    
                    # –ó–∞–º–µ–Ω—è–µ–º sentiment –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–º —à–∞–≥–µ –æ–∫–Ω–∞
                    enhanced_market_state[-1, sentiment_column_index] = enhanced_sentiment
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º market_state, —Å–≥–ª–∞–∂–∏–≤–∞—è –µ–≥–æ –æ–±—Ä–∞—Ç–Ω–æ –≤ 1D –º–∞—Å—Å–∏–≤
                    market_state = enhanced_market_state.flatten()
                    logger.debug(f"üß† Sentiment —É–ª—É—á—à–µ–Ω –¥–æ {enhanced_sentiment:.3f} —Å confidence {confidence:.2f}")
                else:
                    logger.debug(f"‚ö†Ô∏è Sentiment –Ω–µ —É–ª—É—á—à–µ–Ω: confidence {confidence:.2f} < {CONFIDENCE_THRESHOLD}")
            else:
                logger.debug("‚ö†Ô∏è –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä sentiment –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
                
        except Exception as e:
            # –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π market_state
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π sentiment: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π.")
        
        # 3. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Ä—Ç—Ñ–µ–ª—è
        current_price = self.data['GOLD_Close'].iloc[self.current_step]
        unrealized_pnl = 0
        if self.position == 1 and self.entry_price > 0:
            unrealized_pnl = (current_price - self.entry_price) / self.entry_price
        
        portfolio_state = np.array([
            self.balance / self.initial_balance,  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –±–∞–ª–∞–Ω—Å
            float(self.position),                 # –¢–µ–∫—É—â–∞—è –ø–æ–∑–∏—Ü–∏—è (0 –∏–ª–∏ 1)
            self.position_size / max(1.0, self.balance / current_price) if current_price > 0 else 0,  # –†–∞–∑–º–µ—Ä –ø–æ–∑–∏—Ü–∏–∏
            unrealized_pnl                       # –ù–µ—Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π P&L
        ], dtype=np.float32)
        
        # 4. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è
        observation = np.concatenate([market_state, portfolio_state]).astype(np.float32)
        
        # 5. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN/inf –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º
        if np.any(np.isnan(observation)) or np.any(np.isinf(observation)):
            logger.warning("‚ö†Ô∏è NaN/inf –≤ observation, –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
            observation = np.nan_to_num(observation, nan=0.0, posinf=1.0, neginf=-1.0)
        
        # 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        expected_obs_size = self.observation_space.shape[0]
        if observation.shape[0] != expected_obs_size:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ observation! –û–∂–∏–¥–∞–ª–æ—Å—å {expected_obs_size}, –ø–æ–ª—É—á–µ–Ω–æ {observation.shape[0]}")
            # –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø—Ä–∞–≤–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
            if observation.shape[0] < expected_obs_size:
                # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
                padding = np.zeros(expected_obs_size - observation.shape[0])
                observation = np.concatenate([observation, padding])
            else:
                # –û–±—Ä–µ–∑–∞–µ–º
                observation = observation[:expected_obs_size]
            logger.warning(f"üîß –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –¥–æ {observation.shape[0]}")
            
        return observation.astype(np.float32)

    def step(self, action):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π step —Å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º sentiment –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–º –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π Sharpe ratio"""
        current_price = self.data['GOLD_Close'].iloc[self.current_step]
        
        # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π portfolio value
        prev_portfolio_value = self.balance
        if self.position == 1:
            prev_portfolio_value += self.position_size * current_price
            
        # === –û–†–ò–ì–ò–ù–ê–õ–¨–ù–ê–Ø –¢–û–†–ì–û–í–ê–Ø –õ–û–ì–ò–ö–ê ===
        base_reward = 0
        
        if action == 1: # Buy
            if self.position <= 0: 
                self.position = 1
                self.position_size = (self.balance * 0.30) / current_price if current_price > 0 else 0
                self.entry_price = current_price
                self.balance -= self.position_size * current_price * self.commission
                self.trade_count += 1
                base_reward -= 1  # –®—Ç—Ä–∞—Ñ –∑–∞ –æ—Ç–∫—Ä—ã—Ç–∏–µ –ø–æ–∑–∏—Ü–∏–∏
                
        elif action == 2: # Sell
            if self.position == 1:
                profit = (current_price - self.entry_price) * self.position_size
                self.balance += profit - (current_price * self.position_size * self.commission)
                
                # –û—Ç–º–µ—á–∞–µ–º —Å–¥–µ–ª–∫—É –∫–∞–∫ –ø—Ä–∏–±—ã–ª—å–Ω—É—é/—É–±—ã—Ç–æ—á–Ω—É—é
                is_profitable = profit > 0
                self.trades_history.append(is_profitable)
                if len(self.trades_history) > 20:  # –•—Ä–∞–Ω–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 20 —Å–¥–µ–ª–æ–∫
                    self.trades_history.pop(0)
                
                base_reward += profit / self.initial_balance * 100
                self.position = 0
                self.position_size = 0

        # Unrealized profit –ø–æ–∫–∞ –ø–æ–∑–∏—Ü–∏—è –æ—Ç–∫—Ä—ã—Ç–∞
        if self.position != 0:
            if (self.current_step + 1) < len(self.data):
                next_price = self.data['GOLD_Close'].iloc[self.current_step + 1]
                price_diff = (next_price - current_price) * self.position
                base_reward += price_diff / current_price * 100

        # === –ù–û–í–ê–Ø SHARPE-–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø REWARD ===
        
        # 1. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–µ–∫—É—â–∏–π portfolio value
        current_portfolio_value = self.balance
        if self.position == 1:
            current_portfolio_value += self.position_size * current_price
            
        self.portfolio_values.append(current_portfolio_value)
        
        # 2. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º return –∑–∞ –ø–µ—Ä–∏–æ–¥
        period_return = 0
        if len(self.portfolio_values) >= 2:
            period_return = (current_portfolio_value - prev_portfolio_value) / prev_portfolio_value
            self.returns_history.append(period_return)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
            if len(self.returns_history) > 168:  # 1 –Ω–µ–¥–µ–ª—è —á–∞—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                self.returns_history.pop(0)
        
        # 3. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º Sharpe ratio
        sharpe_reward = 0
        current_sharpe = 0
        if len(self.returns_history) >= 24:  # –ú–∏–Ω–∏–º—É–º 24 —á–∞—Å–∞ –¥–∞–Ω–Ω—ã—Ö
            returns_array = np.array(self.returns_history)
            excess_returns = returns_array - self.risk_free_rate
            
            if np.std(excess_returns) > 0:
                current_sharpe = np.mean(excess_returns) / np.std(excess_returns) * np.sqrt(24 * 365)
                sharpe_reward = current_sharpe * 12  # –£–≤–µ–ª–∏—á–∏–ª–∏ —Å 10 –¥–æ 12
            
        # 4. üßÆ –ú–ê–¢–ï–ú–ê–¢–ò–ß–ï–°–ö–ò–ô SENTIMENT –ê–ù–ê–õ–ò–ó
        mathematical_sentiment_reward = 0
        enhanced_sentiment = 0
        sentiment_confidence = 0
        
        try:
            if self.sentiment_analyzer is not None:
                from mathematical_sentiment_model import get_enhanced_sentiment
                
                current_data_row = self.data.iloc[self.current_step]
                enhanced_sentiment, sentiment_confidence = get_enhanced_sentiment(
                    self.sentiment_analyzer, current_data_row
                )
                
                # –ù–∞–≥—Ä–∞–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ sentiment
                if action == 1 and enhanced_sentiment > 0.2:  # Buy –ø—Ä–∏ —Å–∏–ª—å–Ω–æ–º –ø–æ–∑–∏—Ç–∏–≤–Ω–æ–º
                    mathematical_sentiment_reward += enhanced_sentiment * sentiment_confidence * 8
                elif action == 2 and enhanced_sentiment < -0.2:  # Sell –ø—Ä–∏ —Å–∏–ª—å–Ω–æ–º –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–º
                    mathematical_sentiment_reward += abs(enhanced_sentiment) * sentiment_confidence * 8
                elif action == 0 and abs(enhanced_sentiment) < 0.15:  # Hold –ø—Ä–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
                    mathematical_sentiment_reward += sentiment_confidence * 2
                elif action == 1 and enhanced_sentiment < -0.3:  # Buy –ø—Ä–æ—Ç–∏–≤ —Å–∏–ª—å–Ω–æ–≥–æ –Ω–µ–≥–∞—Ç–∏–≤–∞
                    mathematical_sentiment_reward -= abs(enhanced_sentiment) * sentiment_confidence * 10
                
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ sentiment: {e}")
            # Fallback –∫ –±–∞–∑–æ–≤–æ–º—É sentiment
            if 'sentiment' in self.data.columns:
                basic_sentiment = self.data['sentiment'].iloc[self.current_step]
                enhanced_sentiment = basic_sentiment
                sentiment_confidence = 0.5
                
                if action == 1 and basic_sentiment > 0.15:
                    mathematical_sentiment_reward += basic_sentiment * 4
                elif action == 2 and basic_sentiment < -0.15:
                    mathematical_sentiment_reward += abs(basic_sentiment) * 4
        
        # 5. Golden Cross —É–º–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞ (—É—Å–∏–ª–µ–Ω–Ω–∞—è)
        golden_cross_reward = 0
        if 'golden_cross_trend' in self.data.columns:
            gc_trend = self.data['golden_cross_trend'].iloc[self.current_step]
            
            # –£—á–∏—Ç—ã–≤–∞–µ–º —Ç–∞–∫–∂–µ ma_spread –¥–ª—è —Å–∏–ª—ã —Å–∏–≥–Ω–∞–ª–∞
            ma_spread = 0
            if 'ma_spread_normalized' in self.data.columns:
                ma_spread = abs(self.data['ma_spread_normalized'].iloc[self.current_step])
            
            spread_multiplier = 1 + (ma_spread * 2)  # –£—Å–∏–ª–∏–≤–∞–µ–º –ø—Ä–∏ —Å–∏–ª—å–Ω–æ–º —Å–ø—Ä–µ–¥–µ
            
            if action == 1 and gc_trend > 0:  # Buy –≤ –±—ã—á—å–µ–º —Ç—Ä–µ–Ω–¥–µ
                golden_cross_reward += 4 * spread_multiplier
            elif action == 2 and gc_trend < 0:  # Sell –≤ –º–µ–¥–≤–µ–∂—å–µ–º —Ç—Ä–µ–Ω–¥–µ  
                golden_cross_reward += 4 * spread_multiplier
            elif action == 1 and gc_trend < 0:  # Buy –ø—Ä–æ—Ç–∏–≤ —Ç—Ä–µ–Ω–¥–∞
                golden_cross_reward -= 6 * spread_multiplier
            elif action == 0 and gc_trend == 0:  # Hold –≤ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
                golden_cross_reward += 1
        
        # 6. Win rate –±–æ–Ω—É—Å (—É–ª—É—á—à–µ–Ω–Ω—ã–π)
        
        
        # 7. Penalty –∑–∞ –≤—ã—Å–æ–∫—É—é –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å (—É–∂–µ—Å—Ç–æ—á–µ–Ω–Ω—ã–π)
        volatility_penalty = 1
        if len(self.returns_history) >= 24:
            hourly_vol = np.std(self.returns_history)
            annual_vol = hourly_vol * np.sqrt(24 * 365)
            if annual_vol > 0.6:  # –ì–æ–¥–æ–≤–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å > 60%
                volatility_penalty = -(annual_vol - 0.6) * 20
        
        # 8. Penalty –∑–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ—Å–∞–¥–∫–∏ (—É–∂–µ—Å—Ç–æ—á–µ–Ω–Ω—ã–π)
        drawdown_penalty = 0
        current_equity = current_portfolio_value
        equity_ratio = current_equity / self.initial_balance
        
        if equity_ratio < 0.8:  # –ü—Ä–æ—Å–∞–¥–∫–∞ > 20%
            drawdown_penalty = -8
        elif equity_ratio < 0.7:  # –ü—Ä–æ—Å–∞–¥–∫–∞ > 30%
            drawdown_penalty = -20
        elif equity_ratio < 0.6:  # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞ > 40%
            drawdown_penalty = -40
        
        # 9. üî• –ù–û–í–´–ô: Momentum consistency reward
        momentum_consistency_reward = 0
        if len(self.returns_history) >= 5:
            recent_returns = self.returns_history[-5:]
            if action == 1 and all(r > 0 for r in recent_returns[-3:]):  # Buy –Ω–∞ –≤–æ—Å—Ö–æ–¥—è—â–µ–º momentum
                momentum_consistency_reward += 3
            elif action == 2 and all(r < 0 for r in recent_returns[-3:]):  # Sell –Ω–∞ –Ω–∏—Å—Ö–æ–¥—è—â–µ–º momentum
                momentum_consistency_reward += 3
        
        # === –ò–¢–û–ì–û–í–ê–Ø –ù–ê–ì–†–ê–î–ê –° –ù–û–í–´–ú–ò –í–ï–°–ê–ú–ò ===
        total_reward = (base_reward * 0.25 +                    # –ë–∞–∑–æ–≤–∞—è –Ω–∞–≥—Ä–∞–¥–∞
                       sharpe_reward * 0.35 +                   # –û—Å–Ω–æ–≤–Ω–æ–π –≤–µ—Å –Ω–∞ Sharpe
                       mathematical_sentiment_reward * 0.15 +   # üßÆ –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π sentiment
                       golden_cross_reward * 0.10 +             # Golden Cross
                            # Win rate –±–æ–Ω—É—Å
                       volatility_penalty * 0.04 +              # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å —à—Ç—Ä–∞—Ñ
                       drawdown_penalty * 0.04 +                # –ü—Ä–æ—Å–∞–¥–∫–∞ —à—Ç—Ä–∞—Ñ
                       momentum_consistency_reward * 0.02)      # üî• Momentum consistency
        
        self.current_step += 1
        done = self.current_step >= len(self.data) - 2 or equity_ratio < 0.4  # –°—Ç–æ–ø –ø—Ä–∏ 60% –ø—Ä–æ—Å–∞–¥–∫–µ
        
        return self._get_observation(), total_reward, done, False, {
            'sharpe_component': sharpe_reward,
            'mathematical_sentiment_component': mathematical_sentiment_reward,
            'enhanced_sentiment': enhanced_sentiment,
            'sentiment_confidence': sentiment_confidence,
            'golden_cross_component': golden_cross_reward,  
            'base_component': base_reward,
            'current_sharpe': current_sharpe,
            'momentum_consistency': momentum_consistency_reward
        }

    def get_portfolio_stats(self):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å Sharpe ratio"""
        current_price = self.data['GOLD_Close'].iloc[self.current_step - 1]
        current_equity = self.balance
        
        if self.position == 1:
            current_equity += self.position_size * current_price
        
        total_return = (current_equity - self.initial_balance) / self.initial_balance
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º Sharpe ratio
        sharpe_ratio = 0
        if len(self.returns_history) >= 10:
            returns_array = np.array(self.returns_history)
            excess_returns = returns_array - self.risk_free_rate
            if np.std(excess_returns) > 0:
                sharpe_ratio = np.mean(excess_returns) / np.std(excess_returns) * np.sqrt(24 * 365)
        
        # Win rate
        win_rate = sum(self.trades_history) / len(self.trades_history) if self.trades_history else 0
        
        return {
            'balance': self.balance,
            'position': self.position,
            'current_equity': current_equity,
            'total_return': total_return,
            'trade_count': self.trade_count,
            'sharpe_ratio': sharpe_ratio,  # üî• –ù–û–í–û–ï
            'win_rate': win_rate,          # üî• –ù–û–í–û–ï
            'max_drawdown': 1 - (current_equity / max(self.portfolio_values)) if self.portfolio_values else 0  # üî• –ù–û–í–û–ï
        }

# --- –ë–ª–æ–∫ 2: PPO –ê–≥–µ–Ω—Ç (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô) ---
class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super(ActorCritic, self).__init__()
        self.actor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim), 
            nn.ReLU(), 
            nn.Linear(hidden_dim, hidden_dim), 
            nn.ReLU(), 
            nn.Linear(hidden_dim, action_dim), 
            nn.Softmax(dim=-1)
        )
        self.critic = nn.Sequential(
            nn.Linear(state_dim, hidden_dim), 
            nn.ReLU(), 
            nn.Linear(hidden_dim, hidden_dim), 
            nn.ReLU(), 
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, state):
        return self.actor(state), self.critic(state)

class Memory:
    def __init__(self): 
        self.actions, self.states, self.logprobs, self.rewards, self.is_terminals = [], [], [], [], []
    
    def clear(self): 
        del self.actions[:]; del self.states[:]; del self.logprobs[:]; del self.rewards[:]; del self.is_terminals[:]

class PPOAgent:
    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, K_epochs=4, eps_clip=0.2):
        self.gamma, self.eps_clip, self.K_epochs = gamma, eps_clip, K_epochs
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        self.policy = ActorCritic(state_dim, action_dim).to(self.device)
        self.policy_old = ActorCritic(state_dim, action_dim).to(self.device)
        
        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)
        self.policy_old.load_state_dict(self.policy.state_dict())
        self.MseLoss = nn.MSELoss()
        
        logger.info(f"ü§ñ PPO Agent –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –Ω–∞ {self.device}")
        
    def select_action(self, state, memory):
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).to(self.device).unsqueeze(0)
            action_probs, _ = self.policy_old(state_tensor)
            dist = Categorical(action_probs)
            action = dist.sample()
            
            memory.states.append(state_tensor)
            memory.actions.append(action)
            memory.logprobs.append(dist.log_prob(action))
            
        return action.item()

    def update(self, memory):
        # –í—ã—á–∏—Å–ª—è–µ–º discounted rewards
        rewards = []
        discounted_reward = 0
        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):
            if is_terminal: 
                discounted_reward = 0
            discounted_reward = reward + (self.gamma * discounted_reward)
            rewards.insert(0, discounted_reward)
            
        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)

        old_states = torch.squeeze(torch.stack(memory.states, dim=0)).to(self.device).detach()
        old_actions = torch.squeeze(torch.stack(memory.actions, dim=0)).to(self.device).detach()
        old_logprobs = torch.squeeze(torch.stack(memory.logprobs, dim=0)).to(self.device).detach()

        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏
        for _ in range(self.K_epochs):
            action_probs, state_values = self.policy(old_states)
            state_values = torch.squeeze(state_values)

            dist = Categorical(action_probs)
            logprobs = dist.log_prob(old_actions)
            dist_entropy = dist.entropy()
            
            ratios = torch.exp(logprobs - old_logprobs.detach())
            advantages = rewards - state_values.detach()
            
            surr1 = ratios * advantages
            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages
            
            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy
            
            self.optimizer.zero_grad()
            loss.mean().backward()
            self.optimizer.step()
            
        self.policy_old.load_state_dict(self.policy.state_dict())
        return loss.mean().item()

# --- –ë–ª–æ–∫ 3: –ì–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è ---
def main():
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è RL Gold Trader —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π reward —Ñ—É–Ω–∫—Ü–∏–µ–π...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    try:
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤
        possible_files = [
            'master_training_dataset.csv',
            'gold_data_with_sentiment_hourly.csv', 
            'automated_gold_data_with_sentiment.csv'
        ]
        
        data_df = None
        for filename in possible_files:
            try:
                data_df = pd.read_csv(filename, index_col=0, parse_dates=True)
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç: {filename}")
                break
            except FileNotFoundError:
                continue
        
        if data_df is None:
            raise FileNotFoundError("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω –Ω–∏ –æ–¥–∏–Ω –∏–∑ –æ–∂–∏–¥–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç–∞")
            
        logger.info(f"üìä –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {data_df.shape}")
        logger.info(f"üìã –ö–æ–ª–æ–Ω–∫–∏: {list(data_df.columns)}")
        logger.info(f"üìÖ –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: {data_df.index.min()} ‚Üí {data_df.index.max()}")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return

    # –°–æ–∑–¥–∞–µ–º —Å—Ä–µ–¥—É
    try:
        env = EnhancedGoldTradingEnvironment(data_df, lookback_window=24)
        state_dim = env.observation_space.shape[0]
        action_dim = env.action_space.n
        
        logger.info(f"üéØ –°—Ä–µ–¥–∞ —Å–æ–∑–¥–∞–Ω–∞:")
        logger.info(f"   State dimension: {state_dim}")
        logger.info(f"   Action dimension: {action_dim}")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ä–µ–¥—ã: {e}")
        return

    # –°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç–∞
    agent = PPOAgent(state_dim, action_dim, lr=1e-4, gamma=0.99)  # –£–º–µ–Ω—å—à–∏–ª–∏ learning rate
    memory = Memory()

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    num_episodes = 200  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è –Ω–∞—á–∞–ª–∞
    update_timestep = 1000  # –û–±–Ω–æ–≤–ª—è–µ–º —á–∞—â–µ
    max_timesteps = 10000
    
    logger.info(f"üéì –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ:")
    logger.info(f"   Episodes: {num_episodes}")
    logger.info(f"   Update frequency: {update_timestep}")
    logger.info(f"   Max timesteps per episode: {max_timesteps}")
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
    timestep = 0
    best_reward = -float('inf')
    best_sharpe = -float('inf')
    
    for episode in range(1, num_episodes + 1):
        state, _ = env.reset()
        episode_reward = 0
        episode_steps = 0
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã reward –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        total_sharpe_component = 0
        total_sentiment_component = 0
        total_golden_cross_component = 0
        total_base_component = 0
        
        try:
            while episode_steps < max_timesteps:
                timestep += 1
                episode_steps += 1
                
                # –í—ã–±–∏—Ä–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
                action = agent.select_action(state, memory)
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º —à–∞–≥
                state, reward, done, _, info = env.step(action)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å
                memory.rewards.append(reward)
                memory.is_terminals.append(done)
                episode_reward += reward
                
                # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º reward
                if info:
                    total_sharpe_component += info.get('sharpe_component', 0)
                    total_sentiment_component += info.get('mathematical_sentiment_component', 0)
                    total_golden_cross_component += info.get('golden_cross_component', 0)
                    total_base_component += info.get('base_component', 0)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–ª–∏—Ç–∏–∫—É
                if timestep % update_timestep == 0:
                    loss = agent.update(memory)
                    memory.clear()
                    logger.info(f"üîÑ Policy updated at timestep {timestep}. Loss: {loss:.4f}")
                
                if done:
                    break
                    
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —ç–ø–∏–∑–æ–¥–µ {episode}: {e}")
            break
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–ø–∏–∑–æ–¥–∞
        stats = env.get_portfolio_stats()
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ª–æ–≥ —Å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ reward
        logger.info(f"Episode {episode:3d} | "
                   f"Reward: {episode_reward:8.2f} | "
                   f"Return: {stats['total_return']*100:6.2f}% | "
                   f"Sharpe: {stats['sharpe_ratio']:6.3f} | "
                   f"WinRate: {stats['win_rate']*100:5.1f}% | "
                   f"Trades: {stats['trade_count']:3d} | "
                   f"Steps: {episode_steps:4d}")
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ reward –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∫–∞–∂–¥—ã–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤
        if episode % 10 == 0:
            logger.info(f"üìä Reward Components Analysis (Episode {episode}):")
            logger.info(f"   üéØ Base: {total_base_component:8.2f}")
            logger.info(f"   üìà Sharpe: {total_sharpe_component:8.2f}")
            logger.info(f"   üß† Sentiment: {total_sentiment_component:8.2f}")
            logger.info(f"   üåü Golden Cross: {total_golden_cross_component:8.2f}")
            logger.info(f"   üí∞ Max Drawdown: {stats['max_drawdown']*100:5.1f}%")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ –æ–±—â–µ–π –Ω–∞–≥—Ä–∞–¥–µ
        if episode_reward > best_reward:
            best_reward = episode_reward
            best_model_path = f"best_rl_gold_model_reward_{datetime.now().strftime('%Y%m%d_%H%M')}.pth"
            torch.save(agent.policy_old.state_dict(), best_model_path)
            logger.info(f"üíæ –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å –ø–æ reward —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {best_model_path}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ Sharpe ratio
        if stats['sharpe_ratio'] > best_sharpe and stats['trade_count'] > 5:
            best_sharpe = stats['sharpe_ratio']
            best_sharpe_model_path = f"best_rl_gold_model_sharpe_{datetime.now().strftime('%Y%m%d_%H%M')}.pth"
            torch.save(agent.policy_old.state_dict(), best_sharpe_model_path)
            logger.info(f"üìà –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å –ø–æ Sharpe —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {best_sharpe_model_path}")

    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    final_model_path = f"final_rl_gold_model_{datetime.now().strftime('%Y%m%d_%H%M')}.pth"
    torch.save(agent.policy_old.state_dict(), final_model_path)
    
    logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    logger.info(f"üíæ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {final_model_path}")
    logger.info(f"üèÜ –õ—É—á—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {best_reward:.2f}")
    logger.info(f"üìà –õ—É—á—à–∏–π Sharpe ratio: {best_sharpe:.3f}")

if __name__ == '__main__':
    main()