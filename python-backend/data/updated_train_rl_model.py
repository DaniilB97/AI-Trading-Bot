#!/usr/bin/env python3
"""
–û–ë–ù–û–í–õ–ï–ù–ù–´–ô —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è RL –∞–≥–µ–Ω—Ç–∞ –ø–æ–¥ –Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Golden Cross, –≥–æ—Ç–æ–≤—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∏ multi-asset –¥–∞–Ω–Ω—ã–µ
"""
import gymnasium as gym
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import logging
from datetime import datetime
from sklearn.preprocessing import StandardScaler
import pickle
from mathematical_sentiment_model import MathematicalSentimentAnalyzer, get_enhanced_sentiment
from typing import Dict, Tuple 
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –î–û–ë–ê–í–ò–¢–¨ –ö–õ–ê–°–° –ú–ï–¢–†–ò–ö
class PerformanceMetrics:
    """–†–∞—Å—á–µ—Ç —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã —Ü–µ–ª–µ–≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π"""
    
    def __init__(self, risk_free_rate: float = 0.02):
        self.risk_free_rate = risk_free_rate
        self.trades = []
        self.equity_curve = []
        self.daily_returns = []
        
    def add_trade(self, entry_price: float, exit_price: float, position_size: float):
        """–î–æ–±–∞–≤–ª—è–µ–º —Å–¥–µ–ª–∫—É"""
        pnl = (exit_price - entry_price) * position_size
        self.trades.append({
            'pnl': pnl,
            'is_profitable': pnl > 0
        })
        
    def add_equity_point(self, equity: float):
        """–î–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–∫—É equity"""
        self.equity_curve.append(equity)
        if len(self.equity_curve) > 1:
            prev_equity = self.equity_curve[-2]
            daily_return = (equity - prev_equity) / prev_equity
            self.daily_returns.append(daily_return)
    
    def calculate_sharpe_ratio(self) -> float:
        """–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –®–∞—Ä–ø–∞"""
        if not self.daily_returns:
            return 0.0
        excess_returns = np.array(self.daily_returns) - (self.risk_free_rate / 252)
        if np.std(excess_returns) == 0:
            return 0.0
        return np.mean(excess_returns) / np.std(excess_returns) * np.sqrt(252)
    
    def calculate_max_drawdown(self) -> float:
        """–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞"""
        if not self.equity_curve:
            return 0.0
        peak = self.equity_curve[0]
        max_dd = 0.0
        for equity in self.equity_curve:
            if equity > peak:
                peak = equity
            drawdown = (peak - equity) / peak
            if drawdown > max_dd:
                max_dd = drawdown
        return -max_dd
    
    def calculate_win_rate(self) -> float:
        """–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö —Å–¥–µ–ª–æ–∫"""
        if not self.trades:
            return 0.0
        profitable = sum(1 for trade in self.trades if trade['is_profitable'])
        return profitable / len(self.trades)
    
    def get_metrics_summary(self) -> Dict[str, float]:
        """–û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏"""
        if not self.equity_curve:
            return {}
        
        total_return = (self.equity_curve[-1] / self.equity_curve[0]) - 1
        annual_return = total_return  # –£–ø—Ä–æ—â–µ–Ω–Ω–æ
        
        return {
            'total_return': total_return,
            'annual_return': annual_return,
            'sharpe_ratio': self.calculate_sharpe_ratio(),
            'max_drawdown': self.calculate_max_drawdown(),
            'win_rate': self.calculate_win_rate(),
            'total_trades': len(self.trades)
        }

# –î–û–ë–ê–í–ò–¢–¨ –§–£–ù–ö–¶–ò–Æ –†–ê–ó–î–ï–õ–ï–ù–ò–Ø –î–ê–ù–ù–´–•
def split_data_by_time(data: pd.DataFrame, train_ratio: float = 0.6, 
                      val_ratio: float = 0.2) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ –≤—Ä–µ–º–µ–Ω–∏"""
    data_sorted = data.sort_index()
    total_rows = len(data_sorted)
    
    train_end = int(total_rows * train_ratio)
    val_end = int(total_rows * (train_ratio + val_ratio))
    
    train_data = data_sorted.iloc[:train_end].copy()
    val_data = data_sorted.iloc[train_end:val_end].copy()
    test_data = data_sorted.iloc[val_end:].copy()
    
    logger.info(f"üìä –î–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}")
    return train_data, val_data, test_data

# --- –ë–ª–æ–∫ 1: –û–ë–ù–û–í–õ–ï–ù–ù–ê–Ø –°—Ä–µ–¥–∞ –¥–ª—è –Ω–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö ---
"""
    –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Å—Ä–µ–¥–∞ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –Ω–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö:
    1. GOLD_Close –≤–º–µ—Å—Ç–æ Close
    2. –ì–æ—Ç–æ–≤—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (–Ω–µ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∑–∞–Ω–æ–≤–æ)
    3. Golden Cross —Å–∏–≥–Ω–∞–ª—ã
    4. Multi-asset –¥–∞–Ω–Ω—ã–µ
    5. –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π sentiment –∞–Ω–∞–ª–∏–∑
"""
class EnhancedGoldTradingEnvironment(gym.Env):
   
    def __init__(self, data, initial_balance=10000, commission=0.001, lookback_window=24):
        super().__init__()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –≤ –Ω–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        required_columns = ['GOLD_Close', 'sentiment']
        missing_columns = [col for col in required_columns if col not in data.columns]
        if missing_columns:
            raise ValueError(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
            
        self.data = data.copy()
        self.initial_balance = initial_balance
        self.commission = commission
        self.lookback_window = lookback_window
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å window_size –¥–ª—è –Ω–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
        self.window_size = lookback_window 
         # –ê–ª–∏–∞—Å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∏—á–∏ –∏–∑ –ì–û–¢–û–í–û–ì–û –¥–∞—Ç–∞—Å–µ—Ç–∞ (–Ω–µ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∑–∞–Ω–æ–≤–æ!)
        self.feature_columns = self._define_feature_columns()
        logger.info(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ñ–∏—á–∏ ({len(self.feature_columns)}): {self.feature_columns}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Ñ–∏—á–µ–π
        self._validate_features()
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        self._prepare_data()
        logger.info("üîß –°–æ–∑–¥–∞–Ω–∏–µ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ MathematicalSentimentAnalyzer –¥–ª—è —Å—Ä–µ–¥—ã...")
        custom_weights = getattr(self, 'sentiment_weights', None)
        custom_reliabilities = getattr(self, 'sentiment_reliabilities', None)
        self.sentiment_analyzer = MathematicalSentimentAnalyzer(custom_weights, custom_reliabilities)

        
        # Gym spaces
        self.action_space = gym.spaces.Discrete(3)  # 0: Hold, 1: Buy, 2: Sell
        
        # –†–∞–∑–º–µ—Ä observation space
        n_features = len(self.feature_columns)
        n_portfolio_state = 4  # –±–∞–ª–∞–Ω—Å, –ø–æ–∑–∏—Ü–∏—è, —Ä–∞–∑–º–µ—Ä, –Ω–µ—Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π PnL
        observation_space_size = n_features * lookback_window + n_portfolio_state
        
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf, shape=(observation_space_size,), dtype=np.float32
        )
        
        logger.info(f"üéØ Environment –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        logger.info(f"   üìä –î–∞–Ω–Ω—ã—Ö: {len(self.data)} –∑–∞–ø–∏—Å–µ–π")
        logger.info(f"   üß† Observation space: {observation_space_size}")
        logger.info(f"   ‚ö° Action space: {self.action_space.n}")

    def _define_feature_columns(self):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∏—á–∏ –∏–∑ –≥–æ—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        # –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
        base_features = [
            'GOLD_Close',           # –û—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–Ω–∞
            'DXY_Close',            # –ò–Ω–¥–µ–∫—Å –¥–æ–ª–ª–∞—Ä–∞
            'VIX_Close',            # –ò–Ω–¥–µ–∫—Å —Å—Ç—Ä–∞—Ö–∞
            'sentiment'             # –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏—è —Ä—ã–Ω–∫–∞
        ]
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (—É–∂–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–µ)
        technical_features = [
            'STOCHk_14_3_3',        # Stochastic %K
            'CCI_14_0.015',         # Commodity Channel Index
            'Price_Change_1',       # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã –∑–∞ 1 —á–∞—Å
            'Price_Change_5',       # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã –∑–∞ 5 —á–∞—Å–æ–≤
            'Volatility_24',        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            'DXY_change',           # –ò–∑–º–µ–Ω–µ–Ω–∏–µ –¥–æ–ª–ª–∞—Ä–∞
            'VIX_change',           # –ò–∑–º–µ–Ω–µ–Ω–∏–µ VIX
        ]
        
        # Golden Cross —Ñ–∏—á–∏ (–ù–û–í–´–ï!)
        golden_cross_features = [
            'golden_cross_trend',    # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞ (-1, 0, 1)
            'ma_spread_normalized',  # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É MA
        ]
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∏—á–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        optional_features = [
            'RSI_20',               # RSI (–µ—Å–ª–∏ –µ—Å—Ç—å)
            'ATR_20',               # Average True Range (–µ—Å–ª–∏ –µ—Å—Ç—å)
            'MA_20',
                                      
        ]
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∏—á–∏
        all_features = base_features + technical_features + golden_cross_features + optional_features
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –µ—Å—Ç—å –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
        available_features = [col for col in all_features if col in self.data.columns]
        
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ñ–∏—á–µ–π: {len(available_features)} –∏–∑ {len(all_features)} –≤–æ–∑–º–æ–∂–Ω—ã—Ö")
        return available_features

    def _validate_features(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –≤—Å–µ—Ö —Ñ–∏—á–µ–π"""
        missing_features = [col for col in self.feature_columns if col not in self.data.columns]
        if missing_features:
            logger.warning(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ñ–∏—á–∏: {missing_features}")
            # –£–±–∏—Ä–∞–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ñ–∏—á–∏
            self.feature_columns = [col for col in self.feature_columns if col in self.data.columns]
        
        logger.info(f"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞. –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏: {len(self.feature_columns)}")

    def _prepare_data(self):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –ø–µ—Ä–µ—Å—á–µ—Ç–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        logger.info("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        # –£–±–∏—Ä–∞–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
        initial_length = len(self.data)
        self.data = self.data.dropna(subset=self.feature_columns)
        final_length = len(self.data)
        
        if final_length < initial_length:
            logger.warning(f"‚ö†Ô∏è –£–¥–∞–ª–µ–Ω–æ {initial_length - final_length} —Å—Ç—Ä–æ–∫ —Å NaN")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏—á–µ–π (–∫—Ä–æ–º–µ —Ü–µ–Ω –∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö)
        self._normalize_features()
        
        logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã: {len(self.data)} –∑–∞–ø–∏—Å–µ–π –≥–æ—Ç–æ–≤–æ –∫ –æ–±—É—á–µ–Ω–∏—é")

    def _normalize_features(self):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏—á–µ–π –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
        logger.info("üìä –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏—á–µ–π...")
        
        # –§–∏—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º (—Ü–µ–Ω—ã –∏ Golden Cross trend)
        skip_normalization = ['GOLD_Close', 'DXY_Close', 'VIX_Close', 'golden_cross_trend']
        
        for col in self.feature_columns:
            if col not in skip_normalization and col in self.data.columns:
                # Z-score –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                mean_val = self.data[col].mean()
                std_val = self.data[col].std()
                
                if std_val > 1e-9:  # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
                    self.data[col] = (self.data[col] - mean_val) / std_val
                    logger.debug(f"   ‚úì –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω {col}: mean={mean_val:.3f}, std={std_val:.3f}")
                else:
                    logger.warning(f"   ‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è {col}: std —Å–ª–∏—à–∫–æ–º –º–∞–ª")
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Ü–µ–Ω (–ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ)
        for price_col in ['GOLD_Close', 'DXY_Close', 'VIX_Close']:
            if price_col in self.data.columns:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
                first_value = self.data[price_col].iloc[0]
                self.data[f'{price_col}_normalized'] = self.data[price_col] / first_value
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ñ–∏—á–∏ (–∑–∞–º–µ–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—É—é —Ü–µ–Ω—É)
                if price_col in self.feature_columns:
                    idx = self.feature_columns.index(price_col)
                    self.feature_columns[idx] = f'{price_col}_normalized'
        
        logger.info("‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

    def reset(self, seed=None):
        """–°–±—Ä–æ—Å —Å—Ä–µ–¥—ã –≤ –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ - –ö–ê–ö –í –û–†–ò–ì–ò–ù–ê–õ–ï"""
        super().reset(seed=seed)
        self.current_step = self.lookback_window
        self.balance = self.initial_balance
        self.position = 0
        self.position_size = 0.0
        self.entry_price = 0.0
        self.trade_count = 0
        self.metrics = PerformanceMetrics() 
        self.metrics.add_equity_point(self.initial_balance)
        
        return self._get_observation(), {}

    # Inside your GoldTradingEnv class in updated_train_rl_model.py

    def _get_observation(self):
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â–µ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ (—Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ä—ã–Ω–∫–∞ + —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Ä—Ç—Ñ–µ–ª—è).
        –†–æ–±–∞—Å—Ç–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—à–∏–±–æ–∫ –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º sentiment.
        """
        # 1. –í–°–ï–ì–î–ê –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∞–∑–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ä—ã–Ω–∫–∞ –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å
        start = max(0, self.current_step - self.lookback_window + 1)
        end = self.current_step + 1
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ä–µ–∑ –¥–∞–Ω–Ω—ã—Ö
        market_data_slice = self.data.iloc[start:end][self.feature_columns]
        market_state = market_data_slice.values.flatten()
        
        # –ï—Å–ª–∏ –≤ –Ω–∞—á–∞–ª–µ —ç–ø–∏–∑–æ–¥–∞ –¥–∞–Ω–Ω—ã—Ö –º–µ–Ω—å—à–µ, —á–µ–º —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞, –¥–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
        expected_size = self.lookback_window * len(self.feature_columns)
        if len(market_state) < expected_size:
            padding = np.zeros(expected_size - len(market_state))
            market_state = np.concatenate([padding, market_state])
            logger.debug(f"üîß –î–æ–±–∞–≤–ª–µ–Ω padding: {len(padding)} –Ω—É–ª–µ–π")
        
        # 2. –¢–ï–ü–ï–†–¨ –ø—ã—Ç–∞–µ–º—Å—è —É–ª—É—á—à–∏—Ç—å state —Å –ø–æ–º–æ—â—å—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏
    
        try:    
            current_data_row = self.data.iloc[self.current_step]
            enhanced_sentiment, confidence = get_enhanced_sentiment(self.sentiment_analyzer, current_data_row)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π sentiment, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —É–≤–µ—Ä–µ–Ω–∞
            CONFIDENCE_THRESHOLD = 0.5 
            if confidence > CONFIDENCE_THRESHOLD and 'sentiment' in self.feature_columns:
                # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é, —á—Ç–æ–±—ã –Ω–µ –∏–∑–º–µ–Ω—è—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                enhanced_market_state = market_state.reshape(self.lookback_window, len(self.feature_columns)).copy()
                
                # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å –∫–æ–ª–æ–Ω–∫–∏ 'sentiment', —á—Ç–æ–±—ã –∑–∞–º–µ–Ω–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ
                sentiment_column_index = self.feature_columns.index('sentiment')
                
                # –ó–∞–º–µ–Ω—è–µ–º sentiment –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–º —à–∞–≥–µ –æ–∫–Ω–∞
                enhanced_market_state[-1, sentiment_column_index] = enhanced_sentiment
                
                # –û–±–Ω–æ–≤–ª—è–µ–º market_state, —Å–≥–ª–∞–∂–∏–≤–∞—è –µ–≥–æ –æ–±—Ä–∞—Ç–Ω–æ –≤ 1D –º–∞—Å—Å–∏–≤
                market_state = enhanced_market_state.flatten()
                logger.debug(f"üß† Sentiment —É–ª—É—á—à–µ–Ω –¥–æ {enhanced_sentiment:.3f} —Å confidence {confidence:.2f}")
            else:
                logger.debug(f"‚ö†Ô∏è Sentiment –Ω–µ —É–ª—É—á—à–µ–Ω: confidence {confidence:.2f} < {CONFIDENCE_THRESHOLD}")
                
        except Exception as e:
            # –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π market_state
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π sentiment: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π.")
        
        # 3. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Ä—Ç—Ñ–µ–ª—è (–ü–†–ê–í–ò–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø)
        current_price = self.data['GOLD_Close'].iloc[self.current_step]
        unrealized_pnl = 0
        if self.position == 1 and self.entry_price > 0:
            unrealized_pnl = (current_price - self.entry_price) / self.entry_price
        
        portfolio_state = np.array([
            self.balance / self.initial_balance,  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –±–∞–ª–∞–Ω—Å
            float(self.position),                 # –¢–µ–∫—É—â–∞—è –ø–æ–∑–∏—Ü–∏—è (0 –∏–ª–∏ 1)
            self.position_size / max(1.0, self.balance / current_price) if current_price > 0 else 0,  # –†–∞–∑–º–µ—Ä –ø–æ–∑–∏—Ü–∏–∏
            unrealized_pnl                       # –ù–µ—Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π P&L
        ], dtype=np.float32)
        
        # 4. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è. –≠—Ç–∞ —Å—Ç—Ä–æ–∫–∞ —Ç–µ–ø–µ—Ä—å –≤—Å–µ–≥–¥–∞ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å.
        observation = np.concatenate([market_state, portfolio_state]).astype(np.float32)
        
        # 5. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN/inf –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º
        if np.any(np.isnan(observation)) or np.any(np.isinf(observation)):
            logger.warning("‚ö†Ô∏è NaN/inf –≤ observation, –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
            observation = np.nan_to_num(observation, nan=0.0, posinf=1.0, neginf=-1.0)
        
        # 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        expected_obs_size = self.observation_space.shape[0]
        if observation.shape[0] != expected_obs_size:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ observation! –û–∂–∏–¥–∞–ª–æ—Å—å {expected_obs_size}, –ø–æ–ª—É—á–µ–Ω–æ {observation.shape[0]}")
            # –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø—Ä–∞–≤–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
            if observation.shape[0] < expected_obs_size:
                # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
                padding = np.zeros(expected_obs_size - observation.shape[0])
                observation = np.concatenate([observation, padding])
            else:
                # –û–±—Ä–µ–∑–∞–µ–º
                observation = observation[:expected_obs_size]
            logger.warning(f"üîß –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –¥–æ {observation.shape[0]}")
            
        return observation.astype(np.float32)

    def step(self, action):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç —à–∞–≥ –≤ —Å—Ä–µ–¥–µ - –û–†–ò–ì–ò–ù–ê–õ–¨–ù–ê–Ø –õ–û–ì–ò–ö–ê + –õ–û–ì–ò–ß–ù–´–ï –£–õ–£–ß–®–ï–ù–ò–Ø"""
        current_price = self.data['GOLD_Close'].iloc[self.current_step]
        reward = 0
        
        if action == 1: # Buy
            if self.position <= 0: 
                self.position = 1
                self.position_size = (self.balance * 0.95) / current_price if current_price > 0 else 0
                self.entry_price = current_price
                self.balance -= self.position_size * current_price * self.commission
                self.trade_count += 1
                reward -= 1  # –û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ô —à—Ç—Ä–∞—Ñ –∑–∞ –æ—Ç–∫—Ä—ã—Ç–∏–µ –ø–æ–∑–∏—Ü–∏–∏
                
        elif action == 2: # Sell (–≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ, —Ç–æ–ª—å–∫–æ –∑–∞–∫—Ä—ã—Ç–∏–µ –ª–æ–Ω–≥–∞)
            if self.position == 1:
                profit = (current_price - self.entry_price) * self.position_size
                self.balance += profit - (current_price * self.position_size * self.commission)
                reward += profit / self.initial_balance * 100  # –û–†–ò–ì–ò–ù–ê–õ–¨–ù–ê–Ø –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ –ø—Ä–∏–±—ã–ª—å
                self.metrics.add_trade(self.entry_price, current_price, self.position_size)
                self.position = 0
                self.position_size = 0

        # –û–†–ò–ì–ò–ù–ê–õ–¨–ù–ê–Ø –õ–û–ì–ò–ö–ê: –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ unrealized profit –ø–æ–∫–∞ –ø–æ–∑–∏—Ü–∏—è –æ—Ç–∫—Ä—ã—Ç–∞
        if self.position != 0:
            if (self.current_step + 1) < len(self.data):
                next_price = self.data['GOLD_Close'].iloc[self.current_step + 1]
                price_diff = (next_price - current_price) * self.position
                reward += price_diff / current_price * 100  # –≠–¢–û –£–ñ–ï –ë–´–õ–û –í –û–†–ò–ì–ò–ù–ê–õ–ï!

        # ‚ú® –õ–û–ì–ò–ß–ù–û–ï –î–û–ë–ê–í–õ–ï–ù–ò–ï 1: Golden Cross —Å–º–∞—Ä—Ç-–Ω–∞–≥—Ä–∞–¥—ã
        if 'golden_cross_trend' in self.data.columns:
            gc_trend = self.data['golden_cross_trend'].iloc[self.current_step]
            
            # –£–º–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –∑–∞ —Ç–æ—Ä–≥–æ–≤–ª—é –ø–æ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–º—É —Ç—Ä–µ–Ω–¥—É
            if action == 1 and gc_trend > 0:  # Buy –≤ –±—ã—á—å–µ–º —Ç—Ä–µ–Ω–¥–µ
                reward += 2  # –ù–µ–±–æ–ª—å—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞
            elif action == 2 and gc_trend < 0:  # Sell –≤ –º–µ–¥–≤–µ–∂—å–µ–º —Ç—Ä–µ–Ω–¥–µ  
                reward += 2  # –ù–µ–±–æ–ª—å—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞
            elif action == 1 and gc_trend < 0:  # Buy –ø—Ä–æ—Ç–∏–≤ —Ç—Ä–µ–Ω–¥–∞
                reward -= 3  # –®—Ç—Ä–∞—Ñ –∑–∞ —Ç–æ—Ä–≥–æ–≤–ª—é –ø—Ä–æ—Ç–∏–≤ —Ç—Ä–µ–Ω–¥–∞

        # ‚ú® –õ–û–ì–ò–ß–ù–û–ï –î–û–ë–ê–í–õ–ï–ù–ò–ï 2: –®—Ç—Ä–∞—Ñ—ã –∑–∞ –æ–ø–∞—Å–Ω—ã–µ –ø—Ä–æ—Å–∞–¥–∫–∏
        current_equity = self.balance
        if self.position == 1:
            current_equity += self.position_size * current_price
        
        equity_ratio = current_equity / self.initial_balance
        if equity_ratio < 0.7:  # –ü—Ä–æ—Å–∞–¥–∫–∞ –±–æ–ª—å—à–µ 30%
            reward -= 10  # –®—Ç—Ä–∞—Ñ –∑–∞ —Ä–∏—Å–∫
        elif equity_ratio < 0.5:  # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞
            reward -= 25  # –ë–æ–ª—å—à–æ–π —à—Ç—Ä–∞—Ñ

        self.current_step += 1
        done = self.current_step >= len(self.data) - 2 or self.balance < self.initial_balance * 0.5  # –û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ï —É—Å–ª–æ–≤–∏—è
        
        # –î–û–ë–ê–í–ò–¢–¨ –¢–†–ï–ö–ò–ù–ì EQUITY
        current_equity = self.balance
        if self.position == 1:
            current_equity += self.position_size * current_price
        self.metrics.add_equity_point(current_equity)

        return self._get_observation(), reward, done, False, {}

    def get_portfolio_stats(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ—Ä—Ç—Ñ–µ–ª—è - –£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø"""
        current_price = self.data['GOLD_Close'].iloc[self.current_step - 1]
        current_equity = self.balance
        
        if self.position == 1:
            current_equity += self.position_size * current_price
        
        total_return = (current_equity - self.initial_balance) / self.initial_balance
        
        advanced_metrics = self.metrics.get_metrics_summary() if hasattr(self, 'metrics') else {}

        basic_stats = {
            'balance': self.balance,
            'position': self.position,
            'current_equity': current_equity,
            'total_return': total_return,
            'trade_count': self.trade_count
        }
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –±–∞–∑–æ–≤—ã–µ –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        return {**basic_stats, **advanced_metrics}

# --- –ë–ª–æ–∫ 2: PPO –ê–≥–µ–Ω—Ç (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô) ---
class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super(ActorCritic, self).__init__()
        self.actor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim), 
            nn.ReLU(), 
            nn.Linear(hidden_dim, hidden_dim), 
            nn.ReLU(), 
            nn.Linear(hidden_dim, action_dim), 
            nn.Softmax(dim=-1)
        )
        self.critic = nn.Sequential(
            nn.Linear(state_dim, hidden_dim), 
            nn.ReLU(), 
            nn.Linear(hidden_dim, hidden_dim), 
            nn.ReLU(), 
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, state):
        return self.actor(state), self.critic(state)

class Memory:
    def __init__(self): 
        self.actions, self.states, self.logprobs, self.rewards, self.is_terminals = [], [], [], [], []
    
    def clear(self): 
        del self.actions[:]; del self.states[:]; del self.logprobs[:]; del self.rewards[:]; del self.is_terminals[:]

class PPOAgent:
    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, K_epochs=4, eps_clip=0.2, hidden_dim=256):
        self.gamma, self.eps_clip, self.K_epochs = gamma, eps_clip, K_epochs
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        self.policy = ActorCritic(state_dim, action_dim, hidden_dim=hidden_dim).to(self.device)
        self.policy_old = ActorCritic(state_dim, action_dim, hidden_dim=hidden_dim).to(self.device)

        
        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)
        self.policy_old.load_state_dict(self.policy.state_dict())
        self.MseLoss = nn.MSELoss()
        
        logger.info(f"ü§ñ PPO Agent –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –Ω–∞ {self.device}")
        
    def select_action(self, state, memory):
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).to(self.device).unsqueeze(0)
            action_probs, _ = self.policy_old(state_tensor)
            dist = Categorical(action_probs)
            action = dist.sample()
            
            memory.states.append(state_tensor)
            memory.actions.append(action)
            memory.logprobs.append(dist.log_prob(action))
            
        return action.item()

    def update(self, memory):
        # –í—ã—á–∏—Å–ª—è–µ–º discounted rewards
        rewards = []
        discounted_reward = 0
        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):
            if is_terminal: 
                discounted_reward = 0
            discounted_reward = reward + (self.gamma * discounted_reward)
            rewards.insert(0, discounted_reward)
            
        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)

        old_states = torch.squeeze(torch.stack(memory.states, dim=0)).to(self.device).detach()
        old_actions = torch.squeeze(torch.stack(memory.actions, dim=0)).to(self.device).detach()
        old_logprobs = torch.squeeze(torch.stack(memory.logprobs, dim=0)).to(self.device).detach()

        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏
        for _ in range(self.K_epochs):
            action_probs, state_values = self.policy(old_states)
            state_values = torch.squeeze(state_values)

            dist = Categorical(action_probs)
            logprobs = dist.log_prob(old_actions)
            dist_entropy = dist.entropy()
            
            ratios = torch.exp(logprobs - old_logprobs.detach())
            advantages = rewards - state_values.detach()
            
            surr1 = ratios * advantages
            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages
            
            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy
            
            self.optimizer.zero_grad()
            loss.mean().backward()
            self.optimizer.step()
            
        self.policy_old.load_state_dict(self.policy.state_dict())
        return loss.mean().item()

# --- –ë–ª–æ–∫ 3: –ì–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è ---
def main():
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è RL Gold Trader —Å train/val/test split...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    try:
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤
        possible_files = [
            'master_training_dataset.csv',
            'gold_data_with_sentiment_hourly.csv', 
            'automated_gold_data_with_sentiment.csv'
        ]
        
        data_df = None
        for filename in possible_files:
            try:
                data_df = pd.read_csv(filename, index_col=0, parse_dates=True)
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç: {filename}")
                break
            except FileNotFoundError:
                continue
        
        if data_df is None:
            raise FileNotFoundError("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω –Ω–∏ –æ–¥–∏–Ω –∏–∑ –æ–∂–∏–¥–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç–∞")
            
        logger.info(f"üìä –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {data_df.shape}")
        logger.info(f"üìã –ö–æ–ª–æ–Ω–∫–∏: {list(data_df.columns)}")
        logger.info(f"üìÖ –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: {data_df.index.min()} ‚Üí {data_df.index.max()}")
        
        train_data, val_data, test_data = split_data_by_time(data_df, train_ratio=0.6, val_ratio=0.2)

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return

    # –°–æ–∑–¥–∞–µ–º —Å—Ä–µ–¥—É
    try:
        env = EnhancedGoldTradingEnvironment(train_data, lookback_window=24)  # –¢–û–õ–¨–ö–û TRAIN!
        state_dim = env.observation_space.shape[0]
        action_dim = env.action_space.n
        
        logger.info(f"üéØ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ {len(train_data)} –∑–∞–ø–∏—Å—è—Ö (training set)")

        logger.info(f"üéØ –°—Ä–µ–¥–∞ —Å–æ–∑–¥–∞–Ω–∞:")
        logger.info(f"   State dimension: {state_dim}")
        logger.info(f"   Action dimension: {action_dim}")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ä–µ–¥—ã: {e}")
        return

    # –°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç–∞
    agent = PPOAgent(state_dim, action_dim, lr=1e-4, gamma=0.99)  # –£–º–µ–Ω—å—à–∏–ª–∏ learning rate
    memory = Memory()

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    num_episodes = 200  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è –Ω–∞—á–∞–ª–∞
    update_timestep = 1000  # –û–±–Ω–æ–≤–ª—è–µ–º —á–∞—â–µ
    max_timesteps = 10000
    
    logger.info(f"üéì –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ:")
    logger.info(f"   Episodes: {num_episodes}")
    logger.info(f"   Update frequency: {update_timestep}")
    logger.info(f"   Max timesteps per episode: {max_timesteps}")
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
    timestep = 0
    best_reward = -float('inf')
    best_val_return = -float('inf')
    
    for episode in range(1, num_episodes + 1):
        state, _ = env.reset()
        episode_reward = 0
        episode_steps = 0
        
        try:
            while episode_steps < max_timesteps:
                timestep += 1
                episode_steps += 1
                
                # –í—ã–±–∏—Ä–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
                action = agent.select_action(state, memory)
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º —à–∞–≥
                state, reward, done, _, _ = env.step(action)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å
                memory.rewards.append(reward)
                memory.is_terminals.append(done)
                episode_reward += reward
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–ª–∏—Ç–∏–∫—É
                if timestep % update_timestep == 0:
                    loss = agent.update(memory)
                    memory.clear()
                    logger.info(f"üîÑ Policy updated at timestep {timestep}. Loss: {loss:.4f}")
                
                if done:
                    break
                    
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —ç–ø–∏–∑–æ–¥–µ {episode}: {e}")
            break
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–ø–∏–∑–æ–¥–∞
        stats = env.get_portfolio_stats()
        
        logger.info(f"Episode {episode:3d} | "
                   f"Reward: {episode_reward:8.2f} | "
                   f"Return: {stats['total_return']*100:6.2f}% | "
                   f"Trades: {stats['trade_count']:3d} | "
                   f"Sharpe: {stats.get('sharpe_ratio', 0):.2f} | "
                   f"MaxDD: {stats.get('max_drawdown', 0)*100:.1f}% | "
                   f"WinRate: {stats.get('win_rate', 0)*100:.1f}%")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if episode % 10 == 0:
            logger.info(f"üîç –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ—Å–ª–µ —ç–ø–∏–∑–æ–¥–∞ {episode}...")
            try:
                # –°–æ–∑–¥–∞–µ–º —Å—Ä–µ–¥—É –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                val_env = EnhancedGoldTradingEnvironment(val_data, lookback_window=24)
                val_state, _ = val_env.reset()
                val_reward = 0
                val_steps = 0
                
                # –ü—Ä–æ–≥–æ–Ω—è–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è
                while val_steps < len(val_data) - 24 - 1:
                    with torch.no_grad():
                        state_tensor = torch.FloatTensor(val_state).to(agent.device).unsqueeze(0)
                        action_probs, _ = agent.policy_old(state_tensor)
                        action = torch.argmax(action_probs, dim=1).item()  # Greedy action
                    
                    val_state, reward, done, _, _ = val_env.step(action)
                    val_reward += reward
                    val_steps += 1
                    
                    if done:
                        break
                
                val_stats = val_env.get_portfolio_stats()
                val_return = val_stats['total_return']
                
                logger.info(f"üìä Validation: Return={val_return*100:.2f}%, "
                           f"Sharpe={val_stats.get('sharpe_ratio', 0):.2f}, "
                           f"Trades={val_stats['trade_count']}")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ validation
                if val_return > best_val_return:
                    best_val_return = val_return
                    best_model_path = f"best_val_model_{val_return:.3f}_{datetime.now().strftime('%Y%m%d_%H%M')}.pth"
                    torch.save(agent.policy_old.state_dict(), best_model_path)
                    logger.info(f"üíæ –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_path}")
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ training reward
        if episode_reward > best_reward:
            best_reward = episode_reward
            train_best_path = f"best_train_model_{episode_reward:.1f}_{datetime.now().strftime('%Y%m%d_%H%M')}.pth"
            torch.save(agent.policy_old.state_dict(), train_best_path)

    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    final_model_path = f"final_rl_gold_model_{datetime.now().strftime('%Y%m%d_%H%M')}.pth"
    torch.save(agent.policy_old.state_dict(), final_model_path)
    
    # –î–û–ë–ê–í–ò–¢–¨ –§–ò–ù–ê–õ–¨–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï
    logger.info("üìù –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ test set...")
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –µ—Å—Ç—å
        if 'best_model_path' in locals():
            agent.policy_old.load_state_dict(torch.load(best_model_path))
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ test set
        test_env = EnhancedGoldTradingEnvironment(test_data, lookback_window=24)
        test_state, _ = test_env.reset()
        test_reward = 0
        test_steps = 0
        
        while test_steps < len(test_data) - 24 - 1:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(test_state).to(agent.device).unsqueeze(0)
                action_probs, _ = agent.policy_old(state_tensor)
                action = torch.argmax(action_probs, dim=1).item()
            
            test_state, reward, done, _, _ = test_env.step(action)
            test_reward += reward
            test_steps += 1
            
            if done:
                break
        
        test_stats = test_env.get_portfolio_stats()
        
        # –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢
        print("\n" + "="*60)
        print("üìä –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –û –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò")
        print("="*60)
        print(f"üìà Test Return:        {test_stats['total_return']*100:6.2f}% | –¶–µ–ª—å: 15.00%")
        print(f"üìä Sharpe Ratio:       {test_stats.get('sharpe_ratio', 0):6.2f} | –¶–µ–ª—å:  2.50")
        print(f"üìâ Max Drawdown:       {test_stats.get('max_drawdown', 0)*100:6.1f}% | –¶–µ–ª—å: -8.0%")
        print(f"üéØ Win Rate:           {test_stats.get('win_rate', 0)*100:6.1f}% | –¶–µ–ª—å: 65.0%")
        print(f"üîÑ Total Trades:       {test_stats['trade_count']:6d}")
        print("="*60)
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
    
    logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    logger.info(f"üíæ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {final_model_path}")
    logger.info(f"üèÜ –õ—É—á—à–∞—è validation –º–æ–¥–µ–ª—å: {locals().get('best_model_path', 'N/A')}")
    logger.info(f"üèÜ –õ—É—á—à–∏–π validation return: {best_val_return:.2%}")

if __name__ == '__main__':
    main()