#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ Random Forest –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ü–µ–Ω—ã ETH.
"""

import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import asyncio
from pathlib import Path
import argparse
import joblib # For saving scikit-learn models
import yfinance as yf # Import yfinance
import pandas_ta as ta # Import pandas_ta

# Add project root to path
sys.path.append(str(Path(__file__).parent))

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
# from sklearn.preprocessing import MinMaxScaler # RF doesn't strictly need it, can be added if desired

from config_loader import config
from eth_analysis_system import TelegramParser, NewsRelevanceClassifier, TechnicalAnalyzer # Assuming TechnicalAnalyzer is in eth_analysis_system

logger = config.logger

class RandomForestTrainer:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ Random Forest –º–æ–¥–µ–ª–∏"""

    def __init__(self):
        self.config = config
        self.logger = logger
        self.technical_feature_names = [] # Will be populated dynamically
        self.news_feature_names = [
            'message_count', 'avg_sentiment', 'max_impact', 
            'total_views', 'total_reactions'
        ]
        self.setup_directories()

    def setup_directories(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
        dirs = ['models', 'data/raw', 'data/processed', 'logs/training_rf']
        for dir_path in dirs:
            Path(dir_path).mkdir(parents=True, exist_ok=True)

    async def collect_telegram_data(self, days_back=30):
        """–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ Telegram"""
        self.logger.info("üì± –ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ Telegram –¥–ª—è Random Forest...")
        news_cache_path = Path(f"data/raw/telegram_news_rf_{days_back}d.csv")
        if news_cache_path.exists():
            self.logger.info(f"üìÇ –ù–∞–π–¥–µ–Ω –∫—ç—à –Ω–æ–≤–æ—Å—Ç–µ–π: {news_cache_path}, –∑–∞–≥—Ä—É–∂–∞–µ–º...")
            news_df = pd.read_csv(news_cache_path)
            news_df['date'] = pd.to_datetime(news_df['date'])
            news_df['text'] = news_df['text'].astype(str) # Ensure text is string
            return news_df

        telegram_config = self.config.get_telegram_config()
        channels = self.config.get_telegram_channels()
        parser = TelegramParser(**telegram_config)
        await parser.connect()
        try:
            news_df = await parser.parse_multiple_channels(channels, days_back=days_back, limit=500)
            if not news_df.empty:
                news_df['text'] = news_df['text'].astype(str)
                news_classifier = NewsRelevanceClassifier()
                impacts = [news_classifier.analyze_impact(text_content) for text_content in news_df['text']]
                news_df['sentiment_score'] = [i['sentiment_score'] for i in impacts]
                news_df['impact_level'] = [i['impact_level'] for i in impacts]
                news_df['impact_score'] = news_df['sentiment_score'].abs() 
                news_df.to_csv(news_cache_path, index=False)
                self.logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(news_df)} –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –∫—ç—à: {news_cache_path}")
            return news_df
        finally:
            await parser.close()

    def prepare_technical_data(self, days_back=60, interval="1h"):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–∞—Å—Ç–µ—Ä-—Å–µ—Ç–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Random Forest."""
        self.logger.info(f"üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–∞—Å—Ç–µ—Ä-—Å–µ—Ç–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö ({interval} –∏–Ω—Ç–µ—Ä–≤–∞–ª) –¥–ª—è Random Forest...")
        
        master_cache_filename = f"data/raw/eth_master_features_rf_{days_back}d_{interval}.csv"
        master_cache_path = Path(master_cache_filename)

        if master_cache_path.exists():
            self.logger.info(f"üìÇ –ù–∞–π–¥–µ–Ω –º–∞—Å—Ç–µ—Ä-–∫—ç—à —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {master_cache_path}, –∑–∞–≥—Ä—É–∂–∞–µ–º...")
            df_master = pd.read_csv(master_cache_path, index_col=0, parse_dates=True)
            df_master.index = pd.to_datetime(df_master.index).tz_localize(None)
            base_ohlcv = ['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']
            self.technical_feature_names = sorted(list(set(
                [col for col in df_master.columns if col not in base_ohlcv + self.news_feature_names + ['target', 'future_close']]
            )))
            self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω –º–∞—Å—Ç–µ—Ä-–∫—ç—à —Å {len(self.technical_feature_names)} —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏.")
            return df_master
        
        self.logger.info("–ú–∞—Å—Ç–µ—Ä-–∫—ç—à –Ω–µ –Ω–∞–π–¥–µ–Ω. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        # 1. Fetch base ETH data
        eth_ticker = yf.Ticker("ETH-USD")
        df_eth_base = eth_ticker.history(period=f"{days_back}d", interval=interval)
        if df_eth_base.empty:
            self.logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ ETH. –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ.")
            raise ValueError("ETH data could not be fetched.")
        df_eth_base.index = pd.to_datetime(df_eth_base.index).tz_localize(None)
        self.logger.info(f"üíæ –ó–∞–≥—Ä—É–∂–µ–Ω—ã —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ ETH: {len(df_eth_base)} –∑–∞–ø–∏—Å–µ–π")
        
        df_master = df_eth_base.copy() # Start with base OHLCV

        # 2. Calculate broad set of indicators using TechnicalAnalyzer
        analyzer = TechnicalAnalyzer()
        # TechnicalAnalyzer.calculate_indicators is expected to add columns to the df
        df_master = analyzer.calculate_indicators(df_master) 
        self.logger.info(f"üíæ –†–∞—Å—Å—á–∏—Ç–∞–Ω—ã –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∏–∑ TechnicalAnalyzer.")

        # 3. Calculate specific notebook indicators using pandas_ta
        required_pta_cols = ['High', 'Low', 'Close', 'Volume', 'Open']
        if not all(col in df_master.columns for col in required_pta_cols):
            missing_cols = [col for col in required_pta_cols if col not in df_master.columns]
            raise ValueError(f"Missing base columns for pandas_ta: {missing_cols}")

        df_master["VWAP_pta"] = ta.vwap(df_master['High'], df_master['Low'], df_master['Close'], df_master['Volume'])
        df_master['RSI_16_pta'] = ta.rsi(df_master['Close'], length=16)
        bbands_pta = ta.bbands(df_master['Close'], length=14, std=2.0)
        df_master = df_master.join(bbands_pta[['BBL_14_2.0', 'BBM_14_2.0', 'BBU_14_2.0']]) # These names are unique
        df_master['ATRr_7_pta'] = ta.atr(df_master['High'], df_master['Low'], df_master['Close'], length=7)

        backcandles = 15
        VWAPsignal = np.zeros(len(df_master), dtype=int)
        if 'VWAP_pta' not in df_master.columns or df_master['VWAP_pta'].isnull().all():
            self.logger.warning("VWAP_pta (pandas_ta) –Ω–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω –∏–ª–∏ –≤—Å–µ NaN, VWAPSignal_pta –±—É–¥–µ—Ç 0.")
            df_master['VWAPSignal_pta'] = 0
        else:
            for row in range(backcandles, len(df_master)):
                upt = 1; dnt = 1
                window_slice_vwap = df_master['VWAP_pta'].iloc[row-backcandles:row+1]
                window_slice_open = df_master['Open'].iloc[row-backcandles:row+1]
                window_slice_close = df_master['Close'].iloc[row-backcandles:row+1]
                if window_slice_vwap.isnull().any() or window_slice_open.isnull().any() or window_slice_close.isnull().any(): continue
                for i_idx in range(len(window_slice_open)):
                    if max(window_slice_open.iloc[i_idx], window_slice_close.iloc[i_idx]) >= window_slice_vwap.iloc[i_idx]: dnt=0
                    if min(window_slice_open.iloc[i_idx], window_slice_close.iloc[i_idx]) <= window_slice_vwap.iloc[i_idx]: upt=0
                if upt==1 and dnt==1: VWAPsignal[row]=3
                elif upt==1: VWAPsignal[row]=2
                elif dnt==1: VWAPsignal[row]=1
            df_master['VWAPSignal_pta'] = VWAPsignal

        TotSignal = np.zeros(len(df_master), dtype=int)
        required_ts_cols = ['VWAPSignal_pta', 'Close', 'BBL_14_2.0', 'RSI_16_pta', 'BBU_14_2.0']
        if not all(col in df_master.columns for col in required_ts_cols) or any(df_master[col].isnull().all() for col in required_ts_cols):
            self.logger.warning(f"–û–¥–Ω–∞ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è TotalSignal_pta –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∏–ª–∏ –≤—Å–µ NaN: {required_ts_cols}. TotalSignal_pta –±—É–¥–µ—Ç 0.")
            df_master['TotalSignal_pta'] = 0
        else:
            for row_idx in range(backcandles, len(df_master)):
                signal_val = 0
                if df_master[required_ts_cols].iloc[row_idx].isnull().any(): continue
                if (df_master['VWAPSignal_pta'].iloc[row_idx]==2 and df_master['Close'].iloc[row_idx] <= df_master['BBL_14_2.0'].iloc[row_idx] and df_master['RSI_16_pta'].iloc[row_idx] < 45): signal_val = 2
                elif (df_master['VWAPSignal_pta'].iloc[row_idx]==1 and df_master['Close'].iloc[row_idx] >= df_master['BBU_14_2.0'].iloc[row_idx] and df_master['RSI_16_pta'].iloc[row_idx] > 55): signal_val = 1
                TotSignal[row_idx] = signal_val
            df_master['TotalSignal_pta'] = TotSignal
        self.logger.info("üíæ –†–∞—Å—Å—á–∏—Ç–∞–Ω—ã –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∏ —Å–∏–≥–Ω–∞–ª—ã –∏–∑ –Ω–æ—É—Ç–±—É–∫–∞ (pandas_ta) —Å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º _pta.")

        # 4. Fetch BTC data
        btc_ticker = yf.Ticker("BTC-USD")
        df_btc = btc_ticker.history(period=f"{days_back}d", interval=interval)
        if df_btc.empty: 
            self.logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ BTC. –ü—Ä–∏–∑–Ω–∞–∫–∏ BTC –±—É–¥—É—Ç –Ω—É–ª–µ–≤—ã–º–∏.")
            df_btc = pd.DataFrame(index=df_master.index)
        else: 
            df_btc.index = pd.to_datetime(df_btc.index).tz_localize(None)
        
        df_btc['BTC_Close'] = df_btc.get('Close', pd.Series(index=df_btc.index, dtype=float))
        df_btc['BTC_Price_change'] = df_btc['BTC_Close'].pct_change()
        
        # 5. Merge master ETH df with BTC data
        df_master = df_master.join(df_btc[['BTC_Close', 'BTC_Price_change']], how='left') # rsuffix='_btc' removed as BTC columns are unique
        
        # 6. Calculate ETH-BTC correlation
        if 'Price_change' not in df_master.columns and 'Close' in df_master.columns: # Price_change for ETH
             df_master['Price_change'] = df_master['Close'].pct_change() 
        
        if 'Price_change' in df_master.columns and 'BTC_Price_change' in df_master.columns:
            df_master['Price_change'] = df_master['Price_change'].fillna(0)
            df_master['BTC_Price_change'] = df_master['BTC_Price_change'].fillna(0)
            df_master['ETH_BTC_corr_3h'] = df_master['Price_change'].rolling(window=3, min_periods=1).corr(df_master['BTC_Price_change'])
            df_master['ETH_BTC_corr_3h'] = df_master['ETH_BTC_corr_3h'].fillna(0)
        else:
            self.logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å ETH-BTC –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é.")
            df_master['ETH_BTC_corr_3h'] = 0

        # 7. Add time-based features
        df_master['hour_of_day'] = df_master.index.hour
        df_master['day_of_week'] = df_master.index.dayofweek
        
        # 8. Fill NaNs robustly
        # Columns that might have leading NaNs due to rolling calculations or pct_change
        cols_to_fill_na = [
            'Price_change', 'VWAP_pta', 'RSI_16_pta', 'BBL_14_2.0', 'BBM_14_2.0', 'BBU_14_2.0', 'ATRr_7_pta',
            'VWAPSignal_pta', 'TotalSignal_pta', 'BTC_Close', 'BTC_Price_change', 'ETH_BTC_corr_3h'
        ]
        # Also include all columns from TechnicalAnalyzer (excluding OHLCV)
        ta_analyzer_cols = [col for col in df_master.columns if col not in df_eth_base.columns and col not in cols_to_fill_na]
        cols_to_fill_na.extend(ta_analyzer_cols)

        for col in cols_to_fill_na:
            if col in df_master.columns:
                df_master[col] = df_master[col].ffill().bfill().fillna(0)
        
        # 9. Dynamically populate self.technical_feature_names
        base_ohlcv = ['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']
        df_master = df_master.dropna(axis=1, how='all') # Drop columns that are entirely NaN (e.g. 'Dividends')
        
        self.technical_feature_names = sorted(list(set(
            [col for col in df_master.columns if col not in base_ohlcv + self.news_feature_names + ['target', 'future_close']]
        )))
        self.logger.info(f"–ò—Ç–æ–≥–æ–≤—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ({len(self.technical_feature_names)}): {self.technical_feature_names}")

        df_master.to_csv(master_cache_path)
        self.logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –º–∞—Å—Ç–µ—Ä-—Å–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {len(df_master)} –∑–∞–ø–∏—Å–µ–π –≤ {master_cache_path}")
        return df_master

    def aggregate_news_features_by_hour(self, news_df: pd.DataFrame, price_index: pd.DatetimeIndex) -> pd.DataFrame:
        if news_df is None or news_df.empty:
            return pd.DataFrame(0, index=price_index, columns=self.news_feature_names)
        news_df['date'] = pd.to_datetime(news_df['date']).dt.tz_localize(None)
        news_df_hourly_agg = []
        for ts_hour_start in price_index:
            ts_hour_end = ts_hour_start + timedelta(hours=1)
            hour_news = news_df[(news_df['date'] >= ts_hour_start) & (news_df['date'] < ts_hour_end)]
            agg_features = {fn: 0 for fn in self.news_feature_names}
            if not hour_news.empty:
                agg_features.update({
                    'message_count': len(hour_news),
                    'avg_sentiment': hour_news['sentiment_score'].mean(),
                    'max_impact': hour_news['impact_score'].abs().max(),
                    'total_views': hour_news['views'].sum(),
                    'total_reactions': hour_news['reactions'].sum()
                })
            agg_features['timestamp_hour_start'] = ts_hour_start
            news_df_hourly_agg.append(agg_features)
        aggregated_df = pd.DataFrame(news_df_hourly_agg)
        if not aggregated_df.empty:
            aggregated_df = aggregated_df.set_index('timestamp_hour_start')
        else:
             return pd.DataFrame(0, index=price_index, columns=self.news_feature_names)
        return aggregated_df.reindex(price_index).fillna(0)[self.news_feature_names]

    def create_features_and_target(self, tech_df: pd.DataFrame, news_df_hourly_agg: pd.DataFrame, prediction_horizon: int = 1):
        self.logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (–≥–æ—Ä–∏–∑–æ–Ω—Ç {prediction_horizon} —á–∞—Å(–∞))...")
        # self.technical_feature_names should be populated by prepare_technical_data
        if not self.technical_feature_names:
             raise ValueError("self.technical_feature_names is empty. Run prepare_technical_data first or load from cache.")

        existing_tech_features = [col for col in self.technical_feature_names if col in tech_df.columns]
        missing_tech_features = [col for col in self.technical_feature_names if col not in tech_df.columns]
        if missing_tech_features:
            self.logger.warning(f"–°–ª–µ–¥—É—é—â–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ self.technical_feature_names –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ tech_df –∏ –±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã: {missing_tech_features}")
        
        # Ensure 'Close' is in tech_df for target calculation, even if not a feature itself
        cols_for_subset = existing_tech_features + ['Close'] if 'Close' not in existing_tech_features else existing_tech_features
        df_tech_subset = tech_df[list(set(cols_for_subset))].copy() # Use set to avoid duplicate 'Close'

        df_tech_subset.index = pd.to_datetime(df_tech_subset.index).tz_localize(None)
        news_df_hourly_agg.index = pd.to_datetime(news_df_hourly_agg.index).tz_localize(None)
        
        combined_df = df_tech_subset.join(news_df_hourly_agg, how='left')
        
        combined_df['future_close'] = combined_df['Close'].shift(-prediction_horizon)
        combined_df['target'] = (combined_df['future_close'] > combined_df['Close']).astype(int)
        
        # Features to use for X are only existing_tech_features + news_feature_names
        feature_columns_for_X = [col for col in (existing_tech_features + self.news_feature_names) if col in combined_df.columns]
        
        # Columns to check for NaNs before dropping: all features used for X plus the target
        dropna_subset_cols = feature_columns_for_X + ['target']
        combined_df.dropna(subset=dropna_subset_cols, inplace=True)
        
        X = combined_df[feature_columns_for_X]
        y = combined_df['target']
        
        self.logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X)} –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è Random Forest. –ü—Ä–∏–∑–Ω–∞–∫–∏ X: {X.columns.tolist()}")
        return X, y

    def train_evaluate_rf(self, X: pd.DataFrame, y: pd.Series):
        self.logger.info("–¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ Random Forest...")
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)
        self.logger.info(f"–†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö: Train={len(X_train)}, Test={len(X_test)}")
        param_grid = {
            'n_estimators': [100], 
            'max_depth': [5, 8, 12], 
            'min_samples_split': [20, 50, 100], 
            'min_samples_leaf': [10, 20, 50]
        }
        rf_model = RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1)
        grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=None, verbose=1, scoring='f1_weighted') # n_jobs=None for GridSearchCV itself if sub-jobs are -1
        grid_search.fit(X_train, y_train)
        best_rf_model = grid_search.best_estimator_
        self.logger.info(f"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã Random Forest: {grid_search.best_params_}")
        y_pred_train = best_rf_model.predict(X_train)
        y_pred_test = best_rf_model.predict(X_test)
        self.logger.info("--- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ---")
        self.logger.info(f"Accuracy: {accuracy_score(y_train, y_pred_train):.4f}")
        self.logger.info(classification_report(y_train, y_pred_train, zero_division=0))
        self.logger.info("--- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –¢–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö ---")
        self.logger.info(f"Accuracy: {accuracy_score(y_test, y_pred_test):.4f}")
        self.logger.info(classification_report(y_test, y_pred_test, zero_division=0))
        self.logger.info("Confusion Matrix:")
        self.logger.info(f"\n{confusion_matrix(y_test, y_pred_test)}")
        
        if not X.empty:
            feature_importances = pd.Series(best_rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)
            self.logger.info("\n--- –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ---")
            self.logger.info(f"\n{feature_importances.head(30)}") # Show more features
        else:
            self.logger.warning("DataFrame X –ø—É—Å—Ç, –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–∞.")

        model_filename = f"models/eth_random_forest_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib"
        joblib.dump(best_rf_model, model_filename)
        self.logger.info(f"–ú–æ–¥–µ–ª—å Random Forest —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_filename}")
        latest_model_path = "models/eth_random_forest_latest.joblib"
        joblib.dump(best_rf_model, latest_model_path)
        self.logger.info(f"–ú–æ–¥–µ–ª—å Random Forest —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω—è—è: {latest_model_path}")
        return best_rf_model

async def main_rf_feature_selection_loop():
    parser = argparse.ArgumentParser(description='Train ETH Random Forest Model with Feature Selection')
    parser.add_argument('--days-back-tech', type=int, default=60, help='Days of historical technical data')
    parser.add_argument('--interval', type=str, default="1h", help='Data interval (e.g., 1h, 5m)')
    parser.add_argument('--skip-news', action='store_true', help='Skip news data collection')
    parser.add_argument('--prediction-horizon', type=int, default=1, help='Prediction horizon for target variable')
    parser.add_argument('--initial-features-to-keep', type=int, default=30, help='Number of top features to start with after initial run')
    parser.add_argument('--feature-pruning-iterations', type=int, default=5, help='Number of pruning iterations')
    parser.add_argument('--features-to_remove_per_iteration', type=int, default=3, help='Number of features to remove per iteration')


    args = parser.parse_args()
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ Random Forest –º–æ–¥–µ–ª–∏ –¥–ª—è ETH —Å –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–º –≤—ã–±–æ—Ä–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    trainer = RandomForestTrainer()

    try:
        logger.info("–≠—Ç–∞–ø 1: –°–±–æ—Ä –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
        # Use asyncio.to_thread for synchronous self.prepare_technical_data
        full_tech_df = await asyncio.to_thread(trainer.prepare_technical_data, days_back=args.days_back_tech, interval=args.interval)
        full_tech_df.index = pd.to_datetime(full_tech_df.index).tz_localize(None)

        logger.info("–§–æ–∫—É—Å–∏—Ä—É–µ–º—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞—Ö, –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –±—É–¥—É—Ç –Ω—É–ª–µ–≤—ã–º–∏.")
        news_data_hourly_agg = pd.DataFrame(0, index=full_tech_df.index, columns=trainer.news_feature_names)
        if not args.skip_news:
            logger.info("–≠—Ç–∞–ø 2 (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ): –°–±–æ—Ä –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
            await trainer.collect_telegram_data(days_back=7) # Using fixed 7 days for news cache for now

        # Initial full feature set for the first run
        current_technical_features = list(trainer.technical_feature_names) # Get all features from prepare_technical_data
        
        best_overall_f1_score = -1
        best_feature_subset = None
        best_model = None

        for iteration in range(args.feature_pruning_iterations + 1): # +1 for initial run with all features
            logger.info(f"\n===== –ò—Ç–µ—Ä–∞—Ü–∏—è –≤—ã–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ {iteration + 1}/{args.feature_pruning_iterations + 1} =====")
            if not current_technical_features:
                logger.warning("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏.")
                break
            
            logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(current_technical_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {current_technical_features}")
            
            # Update trainer's list for create_features_and_target to use current subset
            trainer.technical_feature_names = current_technical_features 
            
            X, y = trainer.create_features_and_target(full_tech_df.copy(), news_data_hourly_agg.copy(), prediction_horizon=args.prediction_horizon)

            if X.empty or y.empty or X.shape[1] == 0:
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ –Ω–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ X. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.")
                break
            
            logger.info(f"–¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ —Å {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏.")
            trained_model_iteration = trainer.train_evaluate_rf(X, y)
            
            # Evaluate based on test F1-score (macro avg)
            # Need to parse classification_report or get metrics from elsewhere
            # For simplicity, let's assume train_evaluate_rf could return test metrics
            # For now, we'll just track the last model. A proper loop would compare performance.
            # This part needs refinement to properly track the best model based on a validation metric.
            # For this example, we'll just use the features from the last successful run.
            
            # Get feature importances from the trained model for pruning
            if hasattr(trained_model_iteration, 'feature_importances_') and not X.empty :
                importances = pd.Series(trained_model_iteration.feature_importances_, index=X.columns)
                
                if iteration == 0 and args.initial_features_to_keep < len(importances) : # Initial Pruning
                     top_features = importances.sort_values(ascending=False).head(args.initial_features_to_keep).index.tolist()
                     current_technical_features = [f for f in top_features if f not in trainer.news_feature_names] # only prune tech features
                     logger.info(f"–ü–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ –¥–æ {len(current_technical_features)} –ª—É—á—à–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.")
                elif iteration < args.feature_pruning_iterations and len(current_technical_features) > args.features_to_remove_per_iteration : # Subsequent Pruning
                    features_to_remove_count = min(args.features_to_remove_per_iteration, len(current_technical_features) -1 ) # ensure at least 1 feature remains
                    if features_to_remove_count > 0:
                        least_important_features = importances.loc[current_technical_features].sort_values(ascending=True).head(features_to_remove_count).index.tolist()
                        current_technical_features = [f for f in current_technical_features if f not in least_important_features]
                        logger.info(f"–£–¥–∞–ª–µ–Ω–æ {len(least_important_features)} –Ω–∞–∏–º–µ–Ω–µ–µ –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {least_important_features}")
                    else:
                        logger.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–µ–∑–∫–∏.")
                        break

                else: # Last iteration or too few features
                    logger.info("–î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –∏–ª–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.")
                    break 
            else:
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –æ–±—Ä–µ–∑–∫–∞ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞.")
                break
        
        logger.info(f"\nüèÅ –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω.")
        logger.info(f"–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä (–ø–æ—Å–ª–µ–¥–Ω–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏): {current_technical_features}")
        # Here you would typically save the model trained with this 'best_feature_subset'
        # For now, the last saved model from train_evaluate_rf is considered 'latest'

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ Random Forest —Å –≤—ã–±–æ—Ä–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}", exc_info=True)
        raise

if __name__ == "__main__":
    asyncio.run(main_rf_feature_selection_loop())
